{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNNs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V85H6Z78e4hK",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "outputId": "76aa078a-62b4-44b7-e46c-3d32b98951df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "!pip3 -qq install --user torch==0.4.1\n",
        "!pip3 -qq install --user bokeh==0.13.0\n",
        "!pip3 -qq install --user gensim==3.6.0\n",
        "!pip3 -qq install --user nltk\n",
        "!pip3 -qq install --user scikit-learn==0.20.2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 519.5MB 47kB/s \n",
            "\u001b[31mERROR: torchvision 0.3.0 has requirement torch>=1.1.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.55 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 16.0MB 1.3MB/s \n",
            "\u001b[?25h  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: The script bokeh is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 1.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "    device = torch.device(\"cpu\")\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "outputId": "c81c80b9-2f74-4e40-b4cb-36cb171186a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPshvuJnZYGW",
        "colab_type": "code",
        "outputId": "ef550390-cb17-4622-e987-73f08fdcfeb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(type(data[0]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNCZJoAhgAI8",
        "colab_type": "code",
        "outputId": "8e529cbf-be63-4e05-d40f-cd30b0158220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "print([x for x,y in data[0]])\n",
        "print(data[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
            "[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uC9YWkYhEHo",
        "colab_type": "code",
        "outputId": "eb0ba015-d34c-41ce-c0e8-7f9dfe4bece1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "only_sents = nltk.corpus.brown.sents()\n",
        "print(only_sents[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jlr-a9uf3Gw",
        "colab_type": "code",
        "outputId": "4bc7d1ec-a71b-43b5-bccc-6e98c63f12a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "def get_OneHot_by_index(index, num_tokens):\n",
        "    vector = [0]*num_tokens\n",
        "    vector[index] = 1\n",
        "    return torch.tensor(vector)\n",
        "\n",
        "class StanfordTreeBank:\n",
        "    '''\n",
        "    Wrapper for accessing Stanford Tree Bank Dataset\n",
        "    https://nlp.stanford.edu/sentiment/treebank.html\n",
        "    \n",
        "    Parses dataset, gives each token and index and provides lookups\n",
        "    from string token to index and back\n",
        "    \n",
        "    Allows to generate random context with sampling strategy described in\n",
        "    word2vec paper:\n",
        "    https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.index_by_token = {} # map of string -> token index\n",
        "        self.token_by_index = []\n",
        "\n",
        "        self.sentences_by_token = {}\n",
        "        self.sentences = []\n",
        "\n",
        "        self.token_freq = {}\n",
        "        \n",
        "        self.token_reject_by_index = None\n",
        "\n",
        "    def load_dataset(self, folder):\n",
        "        filename = os.path.join(folder, \"datasetSentences.txt\")\n",
        "\n",
        "        with open(filename, \"r\", encoding=\"latin1\") as f:\n",
        "            l = f.readline() # skip the first line\n",
        "            \n",
        "            for l in f:\n",
        "                splitted_line = l.strip().split()\n",
        "                words = [w.lower() for w in splitted_line[1:]] # First one is a number\n",
        "                    \n",
        "                self.sentences.append(words)\n",
        "                for i, word in enumerate(words):\n",
        "                    if word in self.token_freq:\n",
        "                        self.token_freq[word] +=1\n",
        "                        self.sentences_by_token[word].append((len(self.sentences) - 1, i))\n",
        "                    else:\n",
        "                        index = len(self.token_by_index)\n",
        "                        self.token_freq[word] = 1\n",
        "                        self.index_by_token[word] = index\n",
        "                        self.token_by_index.append(word)\n",
        "                        self.sentences_by_token[word] = [(len(self.sentences) - 1, i)] \n",
        "        self.compute_token_prob()\n",
        "    \n",
        "    def init_dataset(self, data):    \n",
        "       \n",
        "        for words in data:\n",
        "              words = [w.lower() for w in words] # First one is a number\n",
        "\n",
        "              self.sentences.append(words)\n",
        "              for i, word in enumerate(words):\n",
        "                  if word in self.token_freq:\n",
        "                      self.token_freq[word] +=1\n",
        "                      self.sentences_by_token[word].append((len(self.sentences) - 1, i))\n",
        "                  else:\n",
        "                      index = len(self.token_by_index)\n",
        "                      self.token_freq[word] = 1\n",
        "                      self.index_by_token[word] = index\n",
        "                      self.token_by_index.append(word)\n",
        "                      self.sentences_by_token[word] = [(len(self.sentences) - 1, i)] \n",
        "        self.compute_token_prob()\n",
        "\n",
        "    def compute_token_prob(self):\n",
        "        words_count = np.array([self.token_freq[token] for token in self.token_by_index])\n",
        "        words_freq = words_count / np.sum(words_count)\n",
        "        \n",
        "        # Following sampling strategy from word2vec paper\n",
        "        self.token_reject_by_index = 1- np.sqrt(1e-5/words_freq)\n",
        "    \n",
        "    def check_reject(self, word):\n",
        "        return np.random.rand() > self.token_reject_by_index[self.index_by_token[word]]\n",
        "        \n",
        "    def get_random_context(self, context_length=5):\n",
        "        \"\"\"\n",
        "        Returns tuple of center word and list of context words\n",
        "        \"\"\"\n",
        "        sentence_sampled = []\n",
        "        while len(sentence_sampled) <= 2:\n",
        "            sentence_index = np.random.randint(len(self.sentences)) \n",
        "            sentence = self.sentences[sentence_index]\n",
        "            sentence_sampled = [word for word in sentence if self.check_reject(word)]\n",
        "    \n",
        "        center_word_index = np.random.randint(len(sentence_sampled))\n",
        "        \n",
        "        words_before = sentence_sampled[max(center_word_index - context_length//2,0):center_word_index]\n",
        "        words_after = sentence_sampled[center_word_index+1: center_word_index+1+context_length//2]\n",
        "        \n",
        "        return sentence_sampled[center_word_index], words_before+words_after\n",
        "    \n",
        "    def num_tokens(self):\n",
        "        return len(self.token_by_index)\n",
        "     \n",
        "    def get_OneHot_by_token(self, token):\n",
        "        return get_OneHot_by_index(self.index_by_token[token], self.num_tokens())\n",
        "    \n",
        "    def find_sentence_with_token(self, token, sentence_index, context_length = 5):\n",
        "        for i, x in enumerate(self.sentences_by_token[token]):\n",
        "            if sentence_index == x[0]:\n",
        "                sentence = [word for word in self.sentences[sentence_index] \\\n",
        "                                if self.check_reject(word)]\n",
        "                begin = max(x[1] - context_length//2, 0)\n",
        "                end = min(x[1] + context_length//2, len(sentence))\n",
        "                sequence = sentence[:begin] + sentence[end+1:]\n",
        "                if sequence != []:\n",
        "                    return random.choice(sequence)\n",
        "                break\n",
        "        return None\n",
        "\n",
        "    def get_random_out_context(self, token, num_negatives_samples, context_length = 5):\n",
        "        result = []\n",
        "        while len(result) < num_negatives_samples:\n",
        "            sentence_index = np.random.randint(len(self.sentences))\n",
        "            if token in self.sentences[sentence_index]:\n",
        "              chosen_token = self.find_sentence_with_token(token, sentence_index, context_length)\n",
        "            else:\n",
        "              chosen_token = None\n",
        "            if chosen_token != None:\n",
        "                result.append(chosen_token)\n",
        "            else:\n",
        "                while True:\n",
        "                    chosen_token = random.choice(self.sentences[sentence_index])\n",
        "                    if self.check_reject(chosen_token):\n",
        "                        result.append(chosen_token)\n",
        "                        break\n",
        "        return result\n",
        "    \n",
        "StandfordData = StanfordTreeBank()\n",
        "StandfordData.init_dataset(only_sents)\n",
        "\n",
        "print(\"Num tokens:\", StandfordData.num_tokens())\n",
        "print(\"Num sentences:\", len(StandfordData.sentences))\n",
        "for i in range(5):\n",
        "    center_word, other_words = StandfordData.get_random_context(5)\n",
        "    print(center_word, other_words)\n",
        "    out_context = StandfordData.get_random_out_context(center_word, 5, 5)\n",
        "    print(out_context)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num tokens: 49815\n",
            "Num sentences: 57340\n",
            "village ['plumber', 'catatonia']\n",
            "['arizona', 'its', 'mama', 'skies', 'horse']\n",
            "shirt ['jackboots', 'peasant']\n",
            "['leg', 'murray', '1298', 'legendary', '30-30']\n",
            "coastal ['marine', 'life']\n",
            "['sewage', 'outcome', 'immortal', 'franks', 'twenty-five']\n",
            "dissatisfied ['totally', 'upper']\n",
            "['thus', 'coltsman', 'theirs', 'infant', '168']\n",
            "undressing ['painter', 'nude']\n",
            "['affirmed', 'less', 'moral', 'reliable', 'you']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2K8WW9Olp9-",
        "colab_type": "code",
        "outputId": "a45de8e0-14d3-4809-d08f-d3d98879c182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "num_negative_samples = 10\n",
        "\n",
        "class Word2VecNegativeSampling(Dataset):\n",
        "    '''\n",
        "    PyTorch Dataset for Word2Vec with Negative Sampling.\n",
        "    Accepts StanfordTreebank as data and is able to generate dataset based on\n",
        "    a number of random contexts\n",
        "    '''\n",
        "    def __init__(self, data, num_negative_samples, num_contexts=30000):\n",
        "        '''\n",
        "        Initializes Word2VecNegativeSampling, but doesn't generate the samples yet\n",
        "        (for that, use generate_dataset)\n",
        "        Arguments:\n",
        "        data - StanfordTreebank instace\n",
        "        num_negative_samples - number of negative samples to generate in addition to a positive one\n",
        "        num_contexts - number of random contexts to use when generating a dataset\n",
        "        '''\n",
        "        \n",
        "        # TODO: Implement what you need for other methods!\n",
        "        self.num_contexts = num_contexts\n",
        "        self.data = data\n",
        "        self.num_negative_samples = num_negative_samples\n",
        "        self.dataset = []\n",
        "        \n",
        "    def generate_dataset(self):\n",
        "        '''\n",
        "        Generates dataset samples from random contexts\n",
        "        Note: there will be more samples than contexts because every context\n",
        "        can generate more than one sample\n",
        "        '''\n",
        "        # TODO: Implement generating the dataset\n",
        "        # You should sample num_contexts contexts from the data and turn them into samples\n",
        "        # Note you will have several samples from one context\n",
        "        del self.dataset\n",
        "        self.dataset = []\n",
        "        center, contexts = 0, 0\n",
        "        for i in range(self.num_contexts):\n",
        "            del center, contexts\n",
        "            center, contexts = self.data.get_random_context()\n",
        "            #print(center, contexts)\n",
        "            for context in contexts:\n",
        "                self.dataset.append((center, [context]+self.data.get_random_out_context(center, \\\n",
        "                                    self.num_negative_samples)))\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns total number of samples\n",
        "        '''\n",
        "        # TODO: Return the number of samples\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        Returns i-th sample\n",
        "        \n",
        "        Return values:\n",
        "        input_vector - index of the input word (not torch.Tensor!)\n",
        "        output_indices - torch.Tensor of indices of the target words. Should be 1+num_negative_samples.\n",
        "        output_target - torch.Tensor with float targets for the training. Should be the same size as output_indices\n",
        "                        and have 1 for the context word and 0 everywhere else\n",
        "        '''\n",
        "        # TODO: Generate tuple of 3 return arguments for i-th sample\n",
        "        center, words = self.dataset[index]\n",
        "        return self.data.index_by_token[center], torch.tensor([self.data.index_by_token[x]\\\n",
        "                for x in words]), torch.tensor([1.]+[0.]*self.num_negative_samples)\n",
        "    \n",
        "dataset = Word2VecNegativeSampling(StandfordData, num_negative_samples, 10)\n",
        "dataset.generate_dataset()\n",
        "input_vector, output_indices, output_target = dataset[0]\n",
        "\n",
        "print(\"Sample - input: %s, output indices: %s, output target: %s\" % (int(input_vector), output_indices, output_target)) # target should be able to convert to int\n",
        "assert isinstance(output_indices, torch.Tensor)\n",
        "assert output_indices.shape[0] == num_negative_samples+1\n",
        "\n",
        "assert isinstance(output_target, torch.Tensor)\n",
        "assert output_target.shape[0] == num_negative_samples+1\n",
        "assert torch.sum(output_target) == 1.0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample - input: 880, output indices: tensor([ 1391,  4938,  9997, 32069,   406,  2661,  6710,   191,  5337, 45329,\n",
            "          471]), output target: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1Zabo-DACer",
        "colab_type": "code",
        "outputId": "c676d71d-c38d-4984-bac9-198031ac06d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Create the usual PyTorch structures\n",
        "dataset = Word2VecNegativeSampling(StandfordData, num_negative_samples, 30000)\n",
        "dataset.generate_dataset()\n",
        "\n",
        "# As before, we'll be training very small word vectors!\n",
        "wordvec_dim = 100\n",
        "\n",
        "class Word2VecNegativeSamples(nn.Module):\n",
        "    def __init__(self, num_tokens, wordvec_dim):\n",
        "        super(Word2VecNegativeSamples, self).__init__()\n",
        "        self.input = nn.Linear(num_tokens, wordvec_dim, bias=False)\n",
        "        self.output = nn.Linear(wordvec_dim, num_tokens, bias=False)\n",
        "        self.wordvec_dim = wordvec_dim\n",
        "        \n",
        "    def forward(self, input_index_batch, output_indices_batch):\n",
        "        '''\n",
        "        Implements forward pass with negative sampling\n",
        "        \n",
        "        Arguments:\n",
        "        input_index_batch - Tensor of ints, shape: (batch_size, ), indices of input words in the batch\n",
        "        output_indices_batch - Tensor if ints, shape: (batch_size, num_negative_samples+1),\n",
        "                                indices of the target words for every sample\n",
        "                                \n",
        "        Returns:\n",
        "        predictions - Tensor of floats, shape: (batch_size, num_negative_samples+1)\n",
        "        '''\n",
        "        # TODO Implement forward pass\n",
        "        # Hint: You can use for loop to go over all samples on the batch,\n",
        "        # run every sample indivisually and then use\n",
        "        # torch.stack or torch.cat to produce the final result\n",
        "        result = torch.cat(tuple(torch.mm(self.output.weight[output_indices_batch[i], :], \\\n",
        "                                self.input.weight[:, input_index_batch[i]]).view(1, -1) \\\n",
        "                                 for i in range(input_index_batch.shape[0])), dim = 0)\n",
        "        return result\n",
        "    \n",
        "nn_model = Word2VecNegativeSamples(StandfordData.num_tokens(), wordvec_dim).to(device)\n",
        "nn_model.type(FloatTensor)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Word2VecNegativeSamples(\n",
              "  (input): Linear(in_features=49815, out_features=100, bias=False)\n",
              "  (output): Linear(in_features=100, out_features=49815, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vjlu0NXDljX",
        "colab_type": "code",
        "outputId": "8ed39d78-18f8-4c89-c0ac-96a787ddd56e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def extract_word_vectors(nn_model):\n",
        "    '''\n",
        "    Extracts word vectors from the model\n",
        "    \n",
        "    Returns:\n",
        "    input_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
        "    output_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
        "    '''\n",
        "    # TODO: Implement extracting word vectors from param weights\n",
        "    # return tuple of input vectors and output vectos \n",
        "    x1, x2 = nn_model.parameters()\n",
        "    x1, x2 = x1.detach(), x2.detach()\n",
        "    num_tokens = x1.shape[1]\n",
        "    num_dimensions = x1.shape[0]\n",
        "    input_vectors = torch.cat(tuple(x1[:, i].expand(1, -1) for i in range(num_tokens)), dim = 0)\n",
        "    output_vectors = torch.cat(tuple(x2[i, :].expand(1, -1) for i in range(num_tokens)), dim = 0)\n",
        "    return input_vectors, output_vectors\n",
        "\n",
        "untrained_input_vectors, untrained_output_vectors = extract_word_vectors(nn_model)\n",
        "print(untrained_input_vectors.shape, untrained_output_vectors.shape)\n",
        "assert untrained_input_vectors.shape == (StandfordData.num_tokens(), wordvec_dim)\n",
        "assert untrained_output_vectors.shape == (StandfordData.num_tokens(), wordvec_dim)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([49815, 100]) torch.Size([49815, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lQpxZ7RDwsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_neg_sample(model, dataset, train_loader, optimizer, scheduler, num_epochs, save = None):    \n",
        "    '''\n",
        "    Trains word2vec with negative samples on and regenerating dataset every epoch\n",
        "    \n",
        "    Returns:\n",
        "    loss_history, train_history\n",
        "    '''\n",
        "    loss = nn.BCEWithLogitsLoss().type(FloatTensor).to(device)\n",
        "    loss_history = []\n",
        "    train_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        if save != None:\n",
        "            torch.save(model.state_dict(), save)\n",
        "        model.train() # Enter train mode\n",
        "        \n",
        "        dataset.generate_dataset()\n",
        "        num_tokens = model.input.weight.shape[0]\n",
        "\n",
        "        # TODO: Implement training using negative samples\n",
        "        # You can estimate accuracy by comparing prediction values with 0\n",
        "        # And don't forget to step the scheduler!\n",
        "        loss_accum = 0\n",
        "        correct_samples = 0\n",
        "        total_samples = 0\n",
        "        for i_step, (input_indices, output_indices, targets) in enumerate(train_loader):\n",
        "            input_indices, output_indices, targets = \\\n",
        "            input_indices.to(device), output_indices.to(device), targets.to(device)\n",
        "            predictions = model(input_indices.view(-1,1), output_indices)\n",
        "            loss_value = loss(predictions, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "            loss_accum += loss_value.detach()\n",
        "            indices = torch.argmax(predictions.detach(), 1)\n",
        "            correct_samples += torch.sum(indices == output_indices[:, 0].detach())\n",
        "            total_samples += targets.shape[0]\n",
        "        train_accuracy = float(correct_samples)/ total_samples\n",
        "        ave_loss = loss_accum/(i_step + 1)\n",
        "        print(\"Epoch %i, Average loss: %f, Train accuracy: %f\" % (epoch, ave_loss, train_accuracy))\n",
        "        loss_history.append(ave_loss)\n",
        "        train_history.append(train_accuracy)\n",
        "        scheduler.step()\n",
        "        \n",
        "    return loss_history, train_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_fPCMI_KYEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_neg_sample(model, dataset, train_loader):    \n",
        "    '''\n",
        "    Trains word2vec with negative samples on and regenerating dataset every epoch\n",
        "    \n",
        "    Returns:\n",
        "    loss_history, train_history\n",
        "    '''\n",
        "    loss = nn.BCEWithLogitsLoss().type(FloatTensor).to(device)\n",
        "    dataset.generate_dataset()\n",
        "    num_tokens = model.input.weight.shape[0]\n",
        "\n",
        "    # TODO: Implement training using negative samples\n",
        "    # You can estimate accuracy by comparing prediction values with 0\n",
        "    # And don't forget to step the scheduler!\n",
        "    loss_accum = 0\n",
        "    correct_samples = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "      for i_step, (input_indices, output_indices, targets) in enumerate(train_loader):\n",
        "          input_indices, output_indices, targets = \\\n",
        "          input_indices.to(device), output_indices.to(device), targets.to(device)\n",
        "          predictions = model(input_indices.view(-1,1), output_indices).detach()\n",
        "          loss_value = loss(predictions, targets)\n",
        "          loss_accum += loss_value\n",
        "          indices = torch.argmax(predictions, 1)\n",
        "          correct_samples += torch.sum(indices == output_indices[:, 0])\n",
        "          total_samples += targets.shape[0]\n",
        "      test_accuracy = float(correct_samples)/ total_samples\n",
        "      ave_loss = loss_accum/(i_step + 1)\n",
        "      print(\"Tests result, Average loss: %f, Test accuracy: %f\" % (ave_loss, test_accuracy))\n",
        "    \n",
        "    return ave_loss, test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyItIpc_OCxY",
        "colab_type": "code",
        "outputId": "120dec87-7813-4da2-d17f-be9c8c4ebc7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ZVlaqlRC5p",
        "colab_type": "code",
        "outputId": "9bcd7750-7661-41d3-853b-1d15499a58f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "nn_model = Word2VecNegativeSamples(StandfordData.num_tokens(), wordvec_dim).to(device)\n",
        "nn_model.type(FloatTensor)\n",
        "nn_model.load_state_dict(torch.load(\"drive/My Drive/Colab Notebooks/word2vecNS_brown.pth\"))\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n",
        "loss, accuracy = test_neg_sample(nn_model, dataset, train_loader)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests result, Average loss: 0.150861, Test accuracy: 0.001032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shDa3Nz1irxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "e2600fa6-fb4c-48a7-b350-c9b634256c9e"
      },
      "source": [
        "optimizer = optim.SGD(nn_model.parameters(), lr=5., weight_decay=0)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.0)\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n",
        "\n",
        "loss_history4, train_history4 = train_neg_sample(nn_model, dataset, train_loader, optimizer, \\\n",
        "                                                 scheduler, 5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Average loss: 0.175858, Train accuracy: 0.000928\n",
            "Epoch 1, Average loss: 0.175578, Train accuracy: 0.001209\n",
            "Epoch 2, Average loss: 0.174623, Train accuracy: 0.001111\n",
            "Epoch 3, Average loss: 0.174279, Train accuracy: 0.001303\n",
            "Epoch 4, Average loss: 0.173571, Train accuracy: 0.000958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00s50Ob_D1Jd",
        "colab_type": "code",
        "outputId": "ca86ce64-a0e8-452e-8b24-1fee03429ab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "source": [
        "# Finally, let's train the model!\n",
        "# TODO: We use placeholder values for hyperparameters - you will need to find better values!\n",
        "optimizer = optim.SGD(nn_model.parameters(), lr=25., weight_decay=0)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.0)\n",
        "\n",
        "loss_history, train_history = train_neg_sample(nn_model, dataset, train_loader, optimizer, \\\n",
        "                                               scheduler, 60, save = \"drive/My Drive/Colab Notebooks/word2vecNS_brown.pth\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Average loss: 0.219757, Train accuracy: 0.000755\n",
            "Epoch 1, Average loss: 0.218357, Train accuracy: 0.000940\n",
            "Epoch 2, Average loss: 0.218848, Train accuracy: 0.000767\n",
            "Epoch 3, Average loss: 0.219023, Train accuracy: 0.000801\n",
            "Epoch 4, Average loss: 0.219406, Train accuracy: 0.000872\n",
            "Epoch 5, Average loss: 0.219698, Train accuracy: 0.001009\n",
            "Epoch 6, Average loss: 0.219096, Train accuracy: 0.000603\n",
            "Epoch 7, Average loss: 0.216879, Train accuracy: 0.000880\n",
            "Epoch 8, Average loss: 0.219098, Train accuracy: 0.001020\n",
            "Epoch 9, Average loss: 0.218793, Train accuracy: 0.001068\n",
            "Epoch 10, Average loss: 0.217645, Train accuracy: 0.000789\n",
            "Epoch 11, Average loss: 0.218883, Train accuracy: 0.000894\n",
            "Epoch 12, Average loss: 0.218936, Train accuracy: 0.000813\n",
            "Epoch 13, Average loss: 0.218566, Train accuracy: 0.000938\n",
            "Epoch 14, Average loss: 0.216143, Train accuracy: 0.000799\n",
            "Epoch 15, Average loss: 0.217901, Train accuracy: 0.001080\n",
            "Epoch 16, Average loss: 0.217283, Train accuracy: 0.000730\n",
            "Epoch 17, Average loss: 0.219327, Train accuracy: 0.001149\n",
            "Epoch 18, Average loss: 0.217886, Train accuracy: 0.000974\n",
            "Epoch 19, Average loss: 0.216460, Train accuracy: 0.000836\n",
            "Epoch 20, Average loss: 0.218133, Train accuracy: 0.000812\n",
            "Epoch 21, Average loss: 0.218910, Train accuracy: 0.001021\n",
            "Epoch 22, Average loss: 0.217313, Train accuracy: 0.000777\n",
            "Epoch 23, Average loss: 0.216092, Train accuracy: 0.001080\n",
            "Epoch 24, Average loss: 0.216894, Train accuracy: 0.000785\n",
            "Epoch 25, Average loss: 0.217401, Train accuracy: 0.000986\n",
            "Epoch 26, Average loss: 0.218200, Train accuracy: 0.001169\n",
            "Epoch 27, Average loss: 0.218448, Train accuracy: 0.001043\n",
            "Epoch 28, Average loss: 0.217486, Train accuracy: 0.000836\n",
            "Epoch 29, Average loss: 0.217283, Train accuracy: 0.000938\n",
            "Epoch 30, Average loss: 0.217284, Train accuracy: 0.000698\n",
            "Epoch 31, Average loss: 0.216209, Train accuracy: 0.000790\n",
            "Epoch 32, Average loss: 0.215836, Train accuracy: 0.001151\n",
            "Epoch 33, Average loss: 0.218136, Train accuracy: 0.000834\n",
            "Epoch 34, Average loss: 0.218391, Train accuracy: 0.000892\n",
            "Epoch 35, Average loss: 0.217701, Train accuracy: 0.000848\n",
            "Epoch 36, Average loss: 0.217233, Train accuracy: 0.001033\n",
            "Epoch 37, Average loss: 0.218442, Train accuracy: 0.001079\n",
            "Epoch 38, Average loss: 0.217168, Train accuracy: 0.000999\n",
            "Epoch 39, Average loss: 0.216005, Train accuracy: 0.001009\n",
            "Epoch 40, Average loss: 0.216510, Train accuracy: 0.000998\n",
            "Epoch 41, Average loss: 0.218166, Train accuracy: 0.000778\n",
            "Epoch 42, Average loss: 0.217105, Train accuracy: 0.001010\n",
            "Epoch 43, Average loss: 0.216075, Train accuracy: 0.000987\n",
            "Epoch 44, Average loss: 0.216624, Train accuracy: 0.000858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYxSv9tSeicE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28b7a9f5-7b88-4716-820e-facdd12c04eb"
      },
      "source": [
        "optimizer = optim.SGD(nn_model.parameters(), lr=5., weight_decay=0)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.0)\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n",
        "\n",
        "loss_history4, train_history4 = train_neg_sample(nn_model, dataset, train_loader, optimizer, \\\n",
        "                                                 scheduler, 100, save = \"drive/My Drive/Colab Notebooks/word2vecNS_brown.pth\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Average loss: 0.171184, Train accuracy: 0.001221\n",
            "Epoch 1, Average loss: 0.172459, Train accuracy: 0.001044\n",
            "Epoch 2, Average loss: 0.170316, Train accuracy: 0.000998\n",
            "Epoch 3, Average loss: 0.169594, Train accuracy: 0.001130\n",
            "Epoch 4, Average loss: 0.169588, Train accuracy: 0.001056\n",
            "Epoch 5, Average loss: 0.169992, Train accuracy: 0.001124\n",
            "Epoch 6, Average loss: 0.169321, Train accuracy: 0.001088\n",
            "Epoch 7, Average loss: 0.169544, Train accuracy: 0.001138\n",
            "Epoch 8, Average loss: 0.169107, Train accuracy: 0.001042\n",
            "Epoch 9, Average loss: 0.168780, Train accuracy: 0.000893\n",
            "Epoch 10, Average loss: 0.166887, Train accuracy: 0.001087\n",
            "Epoch 11, Average loss: 0.167128, Train accuracy: 0.000824\n",
            "Epoch 12, Average loss: 0.166096, Train accuracy: 0.001304\n",
            "Epoch 13, Average loss: 0.167159, Train accuracy: 0.000929\n",
            "Epoch 14, Average loss: 0.164870, Train accuracy: 0.001208\n",
            "Epoch 15, Average loss: 0.165969, Train accuracy: 0.001196\n",
            "Epoch 16, Average loss: 0.164783, Train accuracy: 0.000661\n",
            "Epoch 17, Average loss: 0.164477, Train accuracy: 0.000869\n",
            "Epoch 18, Average loss: 0.164953, Train accuracy: 0.000963\n",
            "Epoch 19, Average loss: 0.164713, Train accuracy: 0.000916\n",
            "Epoch 20, Average loss: 0.164730, Train accuracy: 0.001000\n",
            "Epoch 21, Average loss: 0.164785, Train accuracy: 0.001090\n",
            "Epoch 22, Average loss: 0.163791, Train accuracy: 0.001031\n",
            "Epoch 23, Average loss: 0.161726, Train accuracy: 0.000963\n",
            "Epoch 24, Average loss: 0.163445, Train accuracy: 0.001145\n",
            "Epoch 25, Average loss: 0.162737, Train accuracy: 0.001158\n",
            "Epoch 26, Average loss: 0.161998, Train accuracy: 0.001220\n",
            "Epoch 27, Average loss: 0.161693, Train accuracy: 0.000940\n",
            "Epoch 28, Average loss: 0.163825, Train accuracy: 0.001227\n",
            "Epoch 29, Average loss: 0.162614, Train accuracy: 0.001076\n",
            "Epoch 30, Average loss: 0.161722, Train accuracy: 0.000869\n",
            "Epoch 31, Average loss: 0.162209, Train accuracy: 0.001401\n",
            "Epoch 32, Average loss: 0.161231, Train accuracy: 0.000904\n",
            "Epoch 33, Average loss: 0.162403, Train accuracy: 0.000756\n",
            "Epoch 34, Average loss: 0.161268, Train accuracy: 0.000780\n",
            "Epoch 35, Average loss: 0.160015, Train accuracy: 0.001147\n",
            "Epoch 36, Average loss: 0.160231, Train accuracy: 0.000800\n",
            "Epoch 37, Average loss: 0.159787, Train accuracy: 0.001148\n",
            "Epoch 38, Average loss: 0.161264, Train accuracy: 0.000939\n",
            "Epoch 39, Average loss: 0.159492, Train accuracy: 0.001402\n",
            "Epoch 40, Average loss: 0.159706, Train accuracy: 0.001346\n",
            "Epoch 41, Average loss: 0.159913, Train accuracy: 0.001077\n",
            "Epoch 42, Average loss: 0.160375, Train accuracy: 0.000880\n",
            "Epoch 43, Average loss: 0.160122, Train accuracy: 0.000999\n",
            "Epoch 44, Average loss: 0.159134, Train accuracy: 0.001067\n",
            "Epoch 45, Average loss: 0.159105, Train accuracy: 0.001232\n",
            "Epoch 46, Average loss: 0.159934, Train accuracy: 0.000929\n",
            "Epoch 47, Average loss: 0.159143, Train accuracy: 0.001021\n",
            "Epoch 48, Average loss: 0.158540, Train accuracy: 0.001149\n",
            "Epoch 49, Average loss: 0.159069, Train accuracy: 0.000996\n",
            "Epoch 50, Average loss: 0.159227, Train accuracy: 0.001010\n",
            "Epoch 51, Average loss: 0.158094, Train accuracy: 0.001195\n",
            "Epoch 52, Average loss: 0.158147, Train accuracy: 0.000929\n",
            "Epoch 53, Average loss: 0.156633, Train accuracy: 0.001102\n",
            "Epoch 54, Average loss: 0.158434, Train accuracy: 0.001564\n",
            "Epoch 55, Average loss: 0.158412, Train accuracy: 0.001020\n",
            "Epoch 56, Average loss: 0.156547, Train accuracy: 0.001031\n",
            "Epoch 57, Average loss: 0.157518, Train accuracy: 0.000940\n",
            "Epoch 58, Average loss: 0.157734, Train accuracy: 0.000893\n",
            "Epoch 59, Average loss: 0.157735, Train accuracy: 0.001390\n",
            "Epoch 60, Average loss: 0.156860, Train accuracy: 0.000998\n",
            "Epoch 61, Average loss: 0.157816, Train accuracy: 0.000894\n",
            "Epoch 62, Average loss: 0.157220, Train accuracy: 0.001159\n",
            "Epoch 63, Average loss: 0.157392, Train accuracy: 0.001100\n",
            "Epoch 64, Average loss: 0.157823, Train accuracy: 0.001140\n",
            "Epoch 65, Average loss: 0.156677, Train accuracy: 0.001101\n",
            "Epoch 66, Average loss: 0.156494, Train accuracy: 0.001030\n",
            "Epoch 67, Average loss: 0.157741, Train accuracy: 0.001193\n",
            "Epoch 68, Average loss: 0.156915, Train accuracy: 0.001250\n",
            "Epoch 69, Average loss: 0.157693, Train accuracy: 0.000870\n",
            "Epoch 70, Average loss: 0.156744, Train accuracy: 0.001197\n",
            "Epoch 71, Average loss: 0.156563, Train accuracy: 0.000987\n",
            "Epoch 72, Average loss: 0.156584, Train accuracy: 0.001113\n",
            "Epoch 73, Average loss: 0.156642, Train accuracy: 0.000939\n",
            "Epoch 74, Average loss: 0.156932, Train accuracy: 0.001253\n",
            "Epoch 75, Average loss: 0.157126, Train accuracy: 0.001287\n",
            "Epoch 76, Average loss: 0.155463, Train accuracy: 0.000857\n",
            "Epoch 77, Average loss: 0.154562, Train accuracy: 0.000996\n",
            "Epoch 78, Average loss: 0.155553, Train accuracy: 0.001033\n",
            "Epoch 79, Average loss: 0.155695, Train accuracy: 0.000766\n",
            "Epoch 80, Average loss: 0.155907, Train accuracy: 0.001090\n",
            "Epoch 81, Average loss: 0.156070, Train accuracy: 0.001185\n",
            "Epoch 82, Average loss: 0.156847, Train accuracy: 0.000906\n",
            "Epoch 83, Average loss: 0.155906, Train accuracy: 0.001032\n",
            "Epoch 84, Average loss: 0.155397, Train accuracy: 0.001211\n",
            "Epoch 85, Average loss: 0.155005, Train accuracy: 0.001237\n",
            "Epoch 86, Average loss: 0.155888, Train accuracy: 0.001384\n",
            "Epoch 87, Average loss: 0.154984, Train accuracy: 0.000977\n",
            "Epoch 88, Average loss: 0.154536, Train accuracy: 0.001137\n",
            "Epoch 89, Average loss: 0.155414, Train accuracy: 0.001242\n",
            "Epoch 90, Average loss: 0.154746, Train accuracy: 0.000901\n",
            "Epoch 91, Average loss: 0.154999, Train accuracy: 0.001065\n",
            "Epoch 92, Average loss: 0.154892, Train accuracy: 0.001276\n",
            "Epoch 93, Average loss: 0.153739, Train accuracy: 0.001134\n",
            "Epoch 94, Average loss: 0.153472, Train accuracy: 0.001009\n",
            "Epoch 95, Average loss: 0.154451, Train accuracy: 0.001007\n",
            "Epoch 96, Average loss: 0.153325, Train accuracy: 0.001091\n",
            "Epoch 97, Average loss: 0.152994, Train accuracy: 0.001349\n",
            "Epoch 98, Average loss: 0.154050, Train accuracy: 0.001205\n",
            "Epoch 99, Average loss: 0.153714, Train accuracy: 0.001252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypAG3_ASufaa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "539b82c8-2a50-4004-e861-ce210259ece3"
      },
      "source": [
        "optimizer = optim.SGD(nn_model.parameters(), lr=0.5, weight_decay=0)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.0)\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n",
        "\n",
        "loss_history5, train_history5 = train_neg_sample(nn_model, dataset, train_loader, optimizer, \\\n",
        "                                                 scheduler, 100, save = \"drive/My Drive/Colab Notebooks/word2vecNS_brown.pth\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Average loss: 0.154394, Train accuracy: 0.000882\n",
            "Epoch 1, Average loss: 0.154269, Train accuracy: 0.001103\n",
            "Epoch 2, Average loss: 0.154926, Train accuracy: 0.001065\n",
            "Epoch 3, Average loss: 0.152971, Train accuracy: 0.001031\n",
            "Epoch 4, Average loss: 0.153413, Train accuracy: 0.001022\n",
            "Epoch 5, Average loss: 0.153359, Train accuracy: 0.001000\n",
            "Epoch 6, Average loss: 0.153423, Train accuracy: 0.001125\n",
            "Epoch 7, Average loss: 0.153425, Train accuracy: 0.001033\n",
            "Epoch 8, Average loss: 0.151458, Train accuracy: 0.000809\n",
            "Epoch 9, Average loss: 0.151967, Train accuracy: 0.001358\n",
            "Epoch 10, Average loss: 0.151801, Train accuracy: 0.001290\n",
            "Epoch 11, Average loss: 0.151921, Train accuracy: 0.001101\n",
            "Epoch 12, Average loss: 0.153254, Train accuracy: 0.001207\n",
            "Epoch 13, Average loss: 0.153335, Train accuracy: 0.001077\n",
            "Epoch 14, Average loss: 0.152562, Train accuracy: 0.001046\n",
            "Epoch 15, Average loss: 0.151354, Train accuracy: 0.001124\n",
            "Epoch 16, Average loss: 0.151488, Train accuracy: 0.001127\n",
            "Epoch 17, Average loss: 0.151973, Train accuracy: 0.001561\n",
            "Epoch 18, Average loss: 0.151479, Train accuracy: 0.001090\n",
            "Epoch 19, Average loss: 0.151019, Train accuracy: 0.001043\n",
            "Epoch 20, Average loss: 0.151318, Train accuracy: 0.001068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "outputId": "21d3725c-9225-4718-a781-02bb2c570377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "outputId": "ef8d3763-5a36-460f-ea0a-930ebcce9f04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "outputId": "2de5c2d9-0dee-4eec-8c04-d51d4deb9cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'CONJ', 'ADV', 'PRT', 'ADJ', '.', 'DET', 'NUM', 'PRON', 'ADP', 'VERB', 'NOUN', 'X'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_WXaqFmaA4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trained_input_vectors, trained_output_vectors = extract_word_vectors(nn_model)\n",
        "trained_vectors = torch.cat((trained_input_vectors, trained_output_vectors), 0)\n",
        "embeddings = np.zeros((len(word2ind), trained_vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    if word != '<pad>':\n",
        "        word = word.lower()\n",
        "        embeddings[ind] = trained_vectors[StandfordData.index_by_token[word]+1].cpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XotbQzhZQpF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "75e827de-442e-47bf-e228-fd8687b3fd29"
      },
      "source": [
        "from sklearn.decomposition import PCA \n",
        "import matplotlib.pyplot as plt\n",
        "def visualize_vectors(input_vectors, output_vectors, title=''):\n",
        "    full_vectors = torch.cat((input_vectors, output_vectors), 0)\n",
        "    wordvec_embedding = PCA(n_components=2).fit_transform(full_vectors.detach().cpu())\n",
        "\n",
        "    # Helpful words form CS244D example\n",
        "    # http://cs224d.stanford.edu/assignment1/index.html\n",
        "    visualize_words = {'green': [\"the\", \"a\", \"an\"], \n",
        "                      'blue': [\",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\"], \n",
        "                      'brown': [\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \n",
        "                              \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\"],\n",
        "                      'orange': [\"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\", \"stupid\"],\n",
        "                      'red': ['tell', 'told', 'said', 'say', 'says', 'tells', 'goes', 'go', 'went']\n",
        "                     }\n",
        "\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.suptitle(title)\n",
        "    for color, words in visualize_words.items():\n",
        "        points = np.array([wordvec_embedding[StandfordData.index_by_token[w]] for w in words])\n",
        "        for i, word in enumerate(words):\n",
        "            plt.text(points[i, 0], points[i, 1], word, color=color,horizontalalignment='center')\n",
        "        plt.scatter(points[:, 0], points[:, 1], c=color, alpha=0.3, s=0.5)\n",
        "\n",
        "visualize_vectors(trained_input_vectors, trained_output_vectors, \"Trained word vectors\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAHOCAYAAAAbukfiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npBwVNxERSTkqbiIiknL+B/F+wmm0D64WAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "outputId": "54c09afc-2537-41c5-9795-c710cd3f5f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAEyCAYAAABH+Yw/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHYdJREFUeJzt3X+QZWV95/H3J8NikR8ElAkhgA7q\noAFiRplSKtEsiuhgUoIposMmMhrW0RIqC3GzYpIt3KgbNTGzxUaxMM4CWeVHNAbWGoMTxJjsBmUQ\n5JcCA6LMLD8mgLJZXRD87h/3aT3Tds/09M+n6fer6laf8z3nOfe53efe++lzznNvqgpJkiT168cW\nugOSJEnaNQObJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktS5\nvRa6A7PtgAMOqBUrVix0NyRJknbruuuu++eqWr679Z50gW3FihVs2bJlobshSZK0W0m+MZX1PCUq\nSZLUOQObJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktS53Qa2\nJBuTPJDk5kHt0iQ3tNvdSW5o9RVJvjtY9uFBm6OT3JRka5Jzk6TVn5pkc5I72s/9Wz1tva1Jbkzy\ngtl/+JIkSf2byhG2C4A1w0JVva6qVlXVKuCTwF8PFt85tqyq3jKonwe8CVjZbmPbPBu4qqpWAle1\neYATBuuub+0lSZKWnN1+l2hVfSHJiomWtaNkrwVetqttJDkI2LeqrmnzFwEnAZ8BTgSObateCHwe\neHurX1RVBVyTZL8kB1XVvbt9VJIkaVZt2Hz7jNqfdfzhs9STpWmm17C9BLi/qu4Y1A5Lcn2Sv0/y\nklY7GNg2WGdbqwEcOAhh9wEHDtrcM0mbnSRZn2RLki07duyYwcORJEnqz0wD2ynAxYP5e4GnV9Xz\ngd8FPp5k36lurB1Nqz3tRFWdX1Wrq2r18uXL97S5JElS13Z7SnQySfYCfh04eqxWVY8Cj7bp65Lc\nCRwObAcOGTQ/pNUA7h871dlOnT7Q6tuBQydpI0mStGTM5Ajby4GvVdUPTnUmWZ5kWZt+JqMBA3e1\nU56PJDmmXfd2KnB5a3YFsK5NrxtXP7WNFj0G+LbXr0mSpKVoKh/rcTHwT8BzkmxLclpbtJadT4cC\n/ApwY/uYj08Ab6mqh9qytwJ/AWwF7mQ04ADgvcDxSe5gFALf2+qbgLva+h9p7SVJkpacqYwSPWWS\n+hsmqH2S0cd8TLT+FuCoCeoPAsdNUC/g9N31T5Ik6cnObzqQJEnqnIFNkiSpcwY2SZKkzhnYJEmS\nOmdgkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6Z2CTJEnq\nnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpnYJMkSeqcgU2SJKlz\nBjZJkqTOGdgkSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzu02sCXZmOSBJDcPau9Msj3JDe32qsGydyTZ\nmuS2JK8c1Ne02tYkZw/qhyX5YqtfmmTvVn9Km9/alq+YrQctSZK0mEzlCNsFwJoJ6huqalW7bQJI\ncgSwFjiytflQkmVJlgEfBE4AjgBOaesCvK9t69nAw8BprX4a8HCrb2jrSZIkLTm7DWxV9QXgoSlu\n70Tgkqp6tKq+DmwFXthuW6vqrqp6DLgEODFJgJcBn2jtLwROGmzrwjb9CeC4tr4kSdKSMpNr2M5I\ncmM7Zbp/qx0M3DNYZ1urTVZ/GvCtqnp8XH2nbbXl327rS5IkLSnTDWznAc8CVgH3Ah+YtR5NQ5L1\nSbYk2bJjx46F7IokSdKsm1Zgq6r7q+qJqvo+8BFGpzwBtgOHDlY9pNUmqz8I7Jdkr3H1nbbVlv90\nW3+i/pxfVauravXy5cun85AkSZK6Na3AluSgwexrgLERpFcAa9sIz8OAlcCXgGuBlW1E6N6MBiZc\nUVUFXA2c3NqvAy4fbGtdmz4Z+FxbX5IkaUnZa3crJLkYOBY4IMk24Bzg2CSrgALuBt4MUFW3JLkM\nuBV4HDi9qp5o2zkDuBJYBmysqlvaXbwduCTJu4HrgY+2+keBv0yyldGgh7UzfrSSJEmL0G4DW1Wd\nMkH5oxPUxtZ/D/CeCeqbgE0T1O/ih6dUh/X/B/zG7vonSZL0ZOc3HUiSJHXOwCZJktQ5A5skSVLn\nDGySJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0z\nsEmSJHXOwCZJktQ5A5skSVLnDGySJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJnTOwSZIkdc7A\nJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktQ5A5skSVLnDGySJEmd221gS7IxyQNJbh7U/iTJ\n15LcmORTSfZr9RVJvpvkhnb78KDN0UluSrI1yblJ0upPTbI5yR3t5/6tnrbe1nY/L5j9hy9JktS/\nqRxhuwBYM662GTiqqp4H3A68Y7Dszqpa1W5vGdTPA94ErGy3sW2eDVxVVSuBq9o8wAmDdde39pIk\nSUvObgNbVX0BeGhc7bNV9XibvQY4ZFfbSHIQsG9VXVNVBVwEnNQWnwhc2KYvHFe/qEauAfZr25Ek\nSVpSZuMatt8GPjOYPyzJ9Un+PslLWu1gYNtgnW2tBnBgVd3bpu8DDhy0uWeSNpIkSUvGXjNpnOQP\ngMeBj7XSvcDTq+rBJEcDf5PkyKlur6oqSU2jH+sZnTbl6U9/+p42lyRJ6tq0j7AleQPwa8BvttOc\nVNWjVfVgm74OuBM4HNjOzqdND2k1gPvHTnW2nw+0+nbg0Ena7KSqzq+q1VW1evny5dN9SJIkSV2a\nVmBLsgb4D8Crq+o7g/ryJMva9DMZDRi4q53yfCTJMW106KnA5a3ZFcC6Nr1uXP3UNlr0GODbg1On\nkiRJS8ZuT4kmuRg4FjggyTbgHEajQp8CbG6fznFNGxH6K8AfJfke8H3gLVU1NmDhrYxGnO7D6Jq3\nseve3gtcluQ04BvAa1t9E/AqYCvwHeCNM3mgkiRJi9VuA1tVnTJB+aOTrPtJ4JOTLNsCHDVB/UHg\nuAnqBZy+u/5JkiQ92flNB5IkSZ0zsEmSJHXOwCZJktQ5A5skSVLnDGySJEmdM7BJkiR1zsAmSZLU\nuRl9l6gkSZqeDZtvn3bbs44/fBZ7osXAI2ySJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJnTOw\nSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktQ5A5skSVLnDGySJEmdM7BJkiR1zsAm\nSZLUOQObJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUuemFNiSbEzyQJKbB7WnJtmc5I72c/9W\nT5Jzk2xNcmOSFwzarGvr35Fk3aB+dJKbWptzk2RX9yFJkrSUTPUI2wXAmnG1s4GrqmolcFWbBzgB\nWNlu64HzYBS+gHOAFwEvBM4ZBLDzgDcN2q3ZzX1IkiQtGVMKbFX1BeChceUTgQvb9IXASYP6RTVy\nDbBfkoOAVwKbq+qhqnoY2Aysacv2raprqqqAi8Zta6L7kCRJWjJmcg3bgVV1b5u+DziwTR8M3DNY\nb1ur7aq+bYL6ru5jJ0nWJ9mSZMuOHTum+XAkSZL6NCuDDtqRsZqNbU3nPqrq/KpaXVWrly9fPpfd\nkCRJmnczCWz3t9OZtJ8PtPp24NDBeoe02q7qh0xQ39V9SJIkLRkzCWxXAGMjPdcBlw/qp7bRoscA\n326nNa8EXpFk/zbY4BXAlW3ZI0mOaaNDTx23rYnuQ5IkacnYayorJbkYOBY4IMk2RqM93wtcluQ0\n4BvAa9vqm4BXAVuB7wBvBKiqh5K8C7i2rfdHVTU2kOGtjEai7gN8pt3YxX1IkiQtGVMKbFV1yiSL\njptg3QJOn2Q7G4GNE9S3AEdNUH9wovuQJElaSvymA0mSpM4Z2CRJkjpnYJMkSerclK5hk/TktGHz\n7TNqf9bxh89STyRJu+IRNkmSpM4Z2CRJkjrnKVFJ0qLn6X092XmETZIkqXMGNkmSpM4Z2CRJkjpn\nYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI65+ewSVpU/LwtSUuRR9gkSZI6Z2CTJEnqnIFNkiSpcwY2\nSZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdgk\nSZI6N+3AluQ5SW4Y3B5JcmaSdybZPqi/atDmHUm2JrktySsH9TWttjXJ2YP6YUm+2OqXJtl7+g9V\nkiRpcZp2YKuq26pqVVWtAo4GvgN8qi3eMLasqjYBJDkCWAscCawBPpRkWZJlwAeBE4AjgFPaugDv\na9t6NvAwcNp0+ytJkrRYzdYp0eOAO6vqG7tY50Tgkqp6tKq+DmwFXthuW6vqrqp6DLgEODFJgJcB\nn2jtLwROmqX+SpIkLRqzFdjWAhcP5s9IcmOSjUn2b7WDgXsG62xrtcnqTwO+VVWPj6v/iCTrk2xJ\nsmXHjh0zfzSSJEkdmXFga9eVvRr4q1Y6D3gWsAq4F/jATO9jd6rq/KpaXVWrly9fPtd3J0mSNK/2\nmoVtnAB8uaruBxj7CZDkI8Cn2+x24NBBu0NajUnqDwL7JdmrHWUbri9JkrRkzMYp0VMYnA5NctBg\n2WuAm9v0FcDaJE9JchiwEvgScC2wso0I3ZvR6dUrqqqAq4GTW/t1wOWz0F9JkqRFZUZH2JL8BHA8\n8OZB+f1JVgEF3D22rKpuSXIZcCvwOHB6VT3RtnMGcCWwDNhYVbe0bb0duCTJu4HrgY/OpL+SJEmL\n0YwCW1X9X0aDA4a11+9i/fcA75mgvgnYNEH9LkajSCVJkpYsv+lAkiSpcwY2SZKkzhnYJEmSOmdg\nkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6Z2CTJEnqnIFN\nkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpnYJMkSeqcgU2SJKlzBjZJ\nkqTOGdgkSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzu210B2Qnkw2bL592m3POv7wWeyJJOnJxCNskiRJ\nnZtxYEtyd5KbktyQZEurPTXJ5iR3tJ/7t3qSnJtka5Ibk7xgsJ11bf07kqwb1I9u29/a2mamfZYk\nSVpMZusI20uralVVrW7zZwNXVdVK4Ko2D3ACsLLd1gPnwSjgAecALwJeCJwzFvLaOm8atFszS32W\nJElaFObqlOiJwIVt+kLgpEH9ohq5BtgvyUHAK4HNVfVQVT0MbAbWtGX7VtU1VVXARYNtSZIkLQmz\nEdgK+GyS65Ksb7UDq+reNn0fcGCbPhi4Z9B2W6vtqr5tgvpOkqxPsiXJlh07dsz08UiSJHVlNkaJ\nvriqtif5GWBzkq8NF1ZVJalZuJ9JVdX5wPkAq1evntP7kiRJmm8zPsJWVdvbzweATzG6Bu3+djqT\n9vOBtvp24NBB80NabVf1QyaoS5IkLRkzCmxJfiLJT41NA68AbgauAMZGeq4DLm/TVwCnttGixwDf\nbqdOrwRekWT/NtjgFcCVbdkjSY5po0NPHWxLkiRpSZjpKdEDgU+1T9rYC/h4Vf1tkmuBy5KcBnwD\neG1bfxPwKmAr8B3gjQBV9VCSdwHXtvX+qKoeatNvBS4A9gE+026SJElLxowCW1XdBfziBPUHgeMm\nqBdw+iTb2ghsnKC+BThqJv2UJElazPymA0mSpM4Z2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdgk\nSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpnYJMk\nSercXgvdAc2PDZtvn1H7s44/fJZ6IkmS9pRH2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6\nZ2CTJEnqnB/rIUnaiR8DJPXHI2ySJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJnZt2YEtyaJKr\nk9ya5JYk/67V35lke5Ib2u1VgzbvSLI1yW1JXjmor2m1rUnOHtQPS/LFVr80yd7T7a8kSdJiNZMj\nbI8Db6uqI4BjgNOTHNGWbaiqVe22CaAtWwscCawBPpRkWZJlwAeBE4AjgFMG23lf29azgYeB02bQ\nX0mSpEVp2oGtqu6tqi+36f8DfBU4eBdNTgQuqapHq+rrwFbghe22taruqqrHgEuAE5MEeBnwidb+\nQuCk6fZXkiRpsZqVa9iSrACeD3yxlc5IcmOSjUn2b7WDgXsGzba12mT1pwHfqqrHx9Unuv/1SbYk\n2bJjx45ZeESSJEn9mPE3HST5SeCTwJlV9UiS84B3AdV+fgD47Znez65U1fnA+QCrV6+uubwv8FPA\nJUnS/JpRYEvyrxiFtY9V1V8DVNX9g+UfAT7dZrcDhw6aH9JqTFJ/ENgvyV7tKNtwfUmSpCVjJqNE\nA3wU+GpV/dmgftBgtdcAN7fpK4C1SZ6S5DBgJfAl4FpgZRsRujejgQlXVFUBVwMnt/brgMun219J\nkqTFaiZH2H4ZeD1wU5IbWu33GY3yXMXolOjdwJsBquqWJJcBtzIaYXp6VT0BkOQM4EpgGbCxqm5p\n23s7cEmSdwPXMwqIkiRJS8q0A1tV/SOQCRZt2kWb9wDvmaC+aaJ2VXUXo1GkkiRJS5bfdCBJktQ5\nA5skSVLnDGySJEmdm/HnsElzZSafd+dn3UmSnkw8wiZJktQ5A5skSVLnDGySJEmdM7BJkiR1zsAm\nSZLUOQObJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktS5vRa6\nA5L0ZLdh8+3TbnvW8YfPYk8kLVYeYZMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6Z2CTJEnqnIFNkiSp\ncwY2SZKkzhnYJEmSOmdgkyRJ6lz3gS3JmiS3Jdma5OyF7o8kSdJ86zqwJVkGfBA4ATgCOCXJEQvb\nK0mSpPnVdWADXghsraq7quox4BLgxAXukyRJ0rzq/cvfDwbuGcxvA160QH2RJEmLyIbNt8+o/VnH\nHz5LPZm5VNVC92FSSU4G1lTVv23zrwdeVFVnjFtvPbC+zT4HuG1eO/qjDgD+eYH7sKfs89xbbP0F\n+zwfFlt/wT7Pl8XW58XWX+ijz8+oquW7W6n3I2zbgUMH84e02k6q6nzg/Pnq1O4k2VJVqxe6H3vC\nPs+9xdZfsM/zYbH1F+zzfFlsfV5s/YXF1efer2G7FliZ5LAkewNrgSsWuE+SJEnzqusjbFX1eJIz\ngCuBZcDGqrplgbslSZI0r7oObABVtQnYtND92EPdnJ7dA/Z57i22/oJ9ng+Lrb9gn+fLYuvzYusv\nLKI+dz3oQJIkSf1fwyZJkrTkGdgkSZI6Z2CboiQ/m+SSJHcmuS7JpiSHJzkyyefa953ekeQ/Jklr\n84Yk30/yvMF2bk6yok3fneSAeej7SUkqyXPb/Iok301yfZKvJvlSkjcMlm1L8mPjtnFDknn70OIk\nT7T7vDnJXyX58Qnq/yPJfkl+odVuSPJQkq+36b+br/6O6/uUf99t+RuS/PlC9HWxGfz9b0nylSRv\nG9tXkxyb5NuDfeGGJK8bTN+XZPtgfu857msl+cBg/t8neWebvqB9zuRw/X9pP1e0tu8eLDsgyffm\nYj/Zk+faoM20X/fmyjSfdzvaY7w1yZvmsG9XJ3nluNqZST7T+jjcZ09ty+9OclOSG5P8fZJnDNqO\n/W2+kuTLSX5plvs76b7b5tcn+Vq7fSnJiwfLdnpfa8/LT7fpBdk3xktyaHufeGqb37/Nz2s/9pSB\nbQraC9GngM9X1bOq6mjgHcCBjD5m5L1V9RzgF4FfAt46aL4N+IN57vJ4pwD/2H6OubOqnl9VP8/o\n41LOTPLGqrob+CbwkrEV2wvgT1XVF+exz9+tqlVVdRTwGPCWCeoPAadX1U2ttorR3+P32vzL57G/\nQ1P+fS9I7xa3sb//kcDxjL5n+JzB8n8Y2xfa7dLBvvFhYMNg2WNz3NdHgV/P9P4p+zrwq4P53wDm\naoT8lJ9rAEn2oc/Xvek87y5t+8axwH9OcuAc9e3idv9Da4E/bn0c7rMXDdZ5aVU9D/g88IeD+tjf\n5hcZvRf98Sz3d9J9N8mvAW8GXlxVz2W0v3w8yc9OcdsL/p5YVfcA5wHvbaX3Aue3979uGdim5qXA\n96rqw2OFqvoKcDjwP6vqs632HeAM4OxB208DRyZ5zjz29weS/CTwYuA0fvQFA4Cqugv4XeB3Wmn8\ni8taRt/julD+AXj2BPV/YvT1Zd2Y5u9b01BVDzD6hpMzxo7udOZxRiPQzppG2+8AX00y9oGerwMu\nm62O7cJUnmv/hs5e92b6vGv70p3AM8YvmyWfAH517KhuO5Lzc+z81Yu7sqvXun2Bh2fYv/F2te++\nndE/xf8MUFVfBi6kBfopWND3xIENwDFJzmS07/zpAvdntwxsU3MUcN0E9SPH16vqTuAnk+zbSt8H\n3g/8/pz2cHInAn9bVbcDDyY5epL1vgw8t01fBpyUZOxjX17HKMTNu9aHE4CbxtWXAcfR3wcpT+f3\nrWlqb8LLgJ9ppZeMO730rAXsHsAHgd9M8tPTaHsJsDbJocATwP+e1Z6NswfPtR5f92b0vEvyTOCZ\nwNa56FxVPQR8idHvF0ah8jKggGeN22dfMsEm1gB/M5jfp637NeAvgHfNQbcn23d/5O8PbGn1qVjo\n90QAqup7wO8xCm5ntvmuGdjmx8cZJfnDFuC+T+GHR8cuYefTBUM/OEJRVfcDNwPHJVkFPF5VN89p\nL3/UPkluYPRC8E3go+Pq9zE6Jb15nvu1O3v8+9asGn9K9M6F7ExVPQJcxI8e1Zno85TG1/6W0Wnf\ntcCls9+7H5ir59p8vu5N93n3uvYYLwbe3ILVXBmeuVjLD/8JHn9K9B8Gba5Osp1R0Bv+0zx2SvS5\njMLcRbN9lHkX++5um06htpDviUMnAPcyOijTve4/OLcTtwAnT1C/FfiVYaH9p/YvVfXI2POnfWPD\nBxgdSp437YLKlwG/kKQYHYkoRv85jfd84KuD+bEXl/tZmKNr323XlkxYz+jC6CsZHYY/d367NrEZ\n/r41De359gTwAPDzC9ydyfwXRkd2/tug9iCw/9hM23d2+gLqqnosyXXA24AjgFfPUf/29LnW1eve\nDJ93l1bVGXPZv4HLgQ1JXgD8eFVdN4WL3F8KfAv4GPCfGJ3S3UlV/VO71mw5o+fBbJpo370VOBr4\n3KB2ND+8xnJs3x7bnyfatxfkPXGoHYw4HjgG+Mckl1TVvQvVn6nwCNvUfA54SpL1Y4U2yuU24MVJ\nXt5q+zB6QXv/BNu4AHg5oyfVfDkZ+MuqekZVraiqQxldzHzocKX2ovGnwH8dlP8aeBWj06ELef3a\nhNp1M78DvG1w6nahzeT3rT2UZDmjgQR/Xh1/Ang7anMZo+urxnye0dGdsZGqbwCunqD5B4C3z/GR\nn12a4Ln2Mfp63VsUz7uq+hdGf+ON7ME/wVX1OHAmcOrYqMahNihsGaOgNKsm2XffD7wvydPa/a9i\ntP9+qC3/PPD6tmwZ8FtMvG9fwPy/JwI/GEh4HqNTod8E/gSvYXtyaG8GrwFentHHetzCaFTOfYyu\nnfjDJLcxuvbjWuBHht63EWnn8sNrbWB0hPPROez6KYxGtw59ktGoomelDXdn9IQ8t6p+8F9UVX2L\n0YWu97frhLpTVdcDNzL56Y/5Nt3f91zvB9OW0cfX/NxC92Ng7NqdW4C/Az7L6MjDmPHXsE10ZHwh\nfAD4wYi7qvo0owv8r2un5H6ZCY42VNUtVXXhvPVyEsPnWlV9l5m97s22ab/OLYCLGY2qHQa28dew\nTTQo4t7WZuzC/rHnwQ2MTpevq6on5qjP4/fdKxiFzv/VrqH7CPBbg6NT7wKeneQrwPWMrgv87xM8\npvnYNybzJuCbVTV2mv9DwM8n+dcL0Jcp86upFkg7OnBDVXU1ylHzL8kG4I6q+tBuV5YkLUkeYVsA\nSV7N6L/rdyx0X7SwknwGeB6j00ySJE3II2ySJEmd8wibJElS5wxskiRJnTOwSZIkdc7AJkmS1DkD\nmyRJUuf+P8W9fOQOdw8jAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "outputId": "6938ad1f-a0b3-41d2-bfcb-07a08f517120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "outputId": "3d8964fe-271d-46b7-99fd-18558efd29ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "outputId": "ed5d80a8-5c3d-4bb0-a36c-9e051531a87d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "outputId": "4621ae5b-a937-4a15-ccff-8f059a6180e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((31, 4), (31, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embeddings, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.tagset_size = tagset_size\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim).from_pretrained(embeddings)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count, bidirectional = True)\n",
        "        self.linear = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.linear(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "outputId": "e2140887-4e32-471d-a30a-75770c61541a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind),\n",
        "    embeddings = torch.tensor(embeddings).to(device).type(FloatTensor)\n",
        ").to(device)\n",
        "\n",
        "X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "predictions = torch.argmax(logits, 2)\n",
        "all_words = int(torch.sum(y_batch != 0))\n",
        "print(all_words)\n",
        "accuracy = float(torch.sum((abs(predictions - y_batch) == 0) * (y_batch != 0)))/ all_words\n",
        "print(accuracy)\n",
        "print(logits.shape, y_batch.shape)\n",
        "print(y_batch)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71\n",
            "0.07042253521126761\n",
            "torch.Size([31, 4, 13]) torch.Size([31, 4])\n",
            "tensor([[ 6,  2, 11, 11],\n",
            "        [11,  5, 10, 11],\n",
            "        [ 9,  8,  6,  9],\n",
            "        [ 6, 10,  4,  6],\n",
            "        [11, 10, 11, 11],\n",
            "        [ 9,  2,  3,  5],\n",
            "        [ 6, 10, 10,  0],\n",
            "        [11,  9,  3,  0],\n",
            "        [ 9,  6,  5,  0],\n",
            "        [ 6, 10,  1,  0],\n",
            "        [11,  1,  8,  0],\n",
            "        [ 2, 10, 10,  0],\n",
            "        [10, 11,  2,  0],\n",
            "        [ 4,  9,  5,  0],\n",
            "        [11, 11,  0,  0],\n",
            "        [ 9, 10,  0,  0],\n",
            "        [11,  6,  0,  0],\n",
            "        [ 1,  4,  0,  0],\n",
            "        [11, 11,  0,  0],\n",
            "        [ 5, 11,  0,  0],\n",
            "        [ 0,  5,  0,  0],\n",
            "        [ 0,  9,  0,  0],\n",
            "        [ 0,  6,  0,  0],\n",
            "        [ 0, 10,  0,  0],\n",
            "        [ 0, 11,  0,  0],\n",
            "        [ 0, 10,  0,  0],\n",
            "        [ 0,  9,  0,  0],\n",
            "        [ 0,  6,  0,  0],\n",
            "        [ 0,  4,  0,  0],\n",
            "        [ 0, 11,  0,  0],\n",
            "        [ 0,  5,  0,  0]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "outputId": "39c36544-6943-4b19-fa89-1d298047053b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = 0).to(device)\n",
        "print(criterion(logits.view(-1, model.tagset_size), y_batch.flatten()))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.5699, device='cuda:0', grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "                predictions = torch.argmax(logits, 2)\n",
        "\n",
        "                loss = criterion(logits.view(-1, model.tagset_size), y_batch.flatten())\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                cur_correct_count, cur_sum_count = float(torch.sum(\\\n",
        "                (predictions == y_batch) * (y_batch != 0))), int(torch.sum(y_batch != 0).item())\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, scheduler, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')\n",
        "        scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "outputId": "230622aa-c789-41b4-d889-b3f20a5d0b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind),\n",
        "    embeddings = torch.tensor(embeddings).to(device).type(FloatTensor)\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0).to(device)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=2., weight_decay=0)\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1)\n",
        "\n",
        "fit(model, criterion, optimizer, scheduler, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.83857, Accuracy = 74.16%: 100%|██████████| 572/572 [00:10<00:00, 55.46it/s]\n",
            "[1 / 50]   Val: Loss = 0.56974, Accuracy = 81.50%: 100%|██████████| 13/13 [00:00<00:00, 33.92it/s]\n",
            "[2 / 50] Train: Loss = 0.46642, Accuracy = 84.75%: 100%|██████████| 572/572 [00:10<00:00, 56.06it/s]\n",
            "[2 / 50]   Val: Loss = 0.43497, Accuracy = 85.62%: 100%|██████████| 13/13 [00:00<00:00, 32.41it/s]\n",
            "[3 / 50] Train: Loss = 0.37916, Accuracy = 87.60%: 100%|██████████| 572/572 [00:10<00:00, 56.09it/s]\n",
            "[3 / 50]   Val: Loss = 0.37773, Accuracy = 87.65%: 100%|██████████| 13/13 [00:00<00:00, 33.08it/s]\n",
            "[4 / 50] Train: Loss = 0.33222, Accuracy = 89.19%: 100%|██████████| 572/572 [00:10<00:00, 54.73it/s]\n",
            "[4 / 50]   Val: Loss = 0.33849, Accuracy = 88.84%: 100%|██████████| 13/13 [00:00<00:00, 33.68it/s]\n",
            "[5 / 50] Train: Loss = 0.30262, Accuracy = 90.19%: 100%|██████████| 572/572 [00:10<00:00, 56.16it/s]\n",
            "[5 / 50]   Val: Loss = 0.31346, Accuracy = 89.65%: 100%|██████████| 13/13 [00:00<00:00, 31.93it/s]\n",
            "[6 / 50] Train: Loss = 0.28052, Accuracy = 90.97%: 100%|██████████| 572/572 [00:10<00:00, 55.93it/s]\n",
            "[6 / 50]   Val: Loss = 0.30214, Accuracy = 90.14%: 100%|██████████| 13/13 [00:00<00:00, 33.70it/s]\n",
            "[7 / 50] Train: Loss = 0.26512, Accuracy = 91.51%: 100%|██████████| 572/572 [00:10<00:00, 56.60it/s]\n",
            "[7 / 50]   Val: Loss = 0.27851, Accuracy = 91.00%: 100%|██████████| 13/13 [00:00<00:00, 34.71it/s]\n",
            "[8 / 50] Train: Loss = 0.25174, Accuracy = 91.98%: 100%|██████████| 572/572 [00:10<00:00, 56.26it/s]\n",
            "[8 / 50]   Val: Loss = 0.27427, Accuracy = 91.10%: 100%|██████████| 13/13 [00:00<00:00, 33.87it/s]\n",
            "[9 / 50] Train: Loss = 0.24199, Accuracy = 92.30%: 100%|██████████| 572/572 [00:10<00:00, 56.04it/s]\n",
            "[9 / 50]   Val: Loss = 0.27176, Accuracy = 91.23%: 100%|██████████| 13/13 [00:00<00:00, 32.57it/s]\n",
            "[10 / 50] Train: Loss = 0.23233, Accuracy = 92.60%: 100%|██████████| 572/572 [00:10<00:00, 55.93it/s]\n",
            "[10 / 50]   Val: Loss = 0.26359, Accuracy = 91.31%: 100%|██████████| 13/13 [00:00<00:00, 34.77it/s]\n",
            "[11 / 50] Train: Loss = 0.22536, Accuracy = 92.85%: 100%|██████████| 572/572 [00:10<00:00, 56.26it/s]\n",
            "[11 / 50]   Val: Loss = 0.25318, Accuracy = 91.80%: 100%|██████████| 13/13 [00:00<00:00, 32.48it/s]\n",
            "[12 / 50] Train: Loss = 0.21844, Accuracy = 93.05%: 100%|██████████| 572/572 [00:10<00:00, 55.92it/s]\n",
            "[12 / 50]   Val: Loss = 0.24821, Accuracy = 91.97%: 100%|██████████| 13/13 [00:00<00:00, 34.01it/s]\n",
            "[13 / 50] Train: Loss = 0.21415, Accuracy = 93.21%: 100%|██████████| 572/572 [00:10<00:00, 59.94it/s]\n",
            "[13 / 50]   Val: Loss = 0.24612, Accuracy = 91.91%: 100%|██████████| 13/13 [00:00<00:00, 32.67it/s]\n",
            "[14 / 50] Train: Loss = 0.20820, Accuracy = 93.40%: 100%|██████████| 572/572 [00:10<00:00, 56.21it/s]\n",
            "[14 / 50]   Val: Loss = 0.24561, Accuracy = 91.92%: 100%|██████████| 13/13 [00:00<00:00, 33.93it/s]\n",
            "[15 / 50] Train: Loss = 0.20516, Accuracy = 93.50%: 100%|██████████| 572/572 [00:10<00:00, 56.25it/s]\n",
            "[15 / 50]   Val: Loss = 0.22882, Accuracy = 92.58%: 100%|██████████| 13/13 [00:00<00:00, 33.24it/s]\n",
            "[16 / 50] Train: Loss = 0.20086, Accuracy = 93.63%: 100%|██████████| 572/572 [00:10<00:00, 56.14it/s]\n",
            "[16 / 50]   Val: Loss = 0.23012, Accuracy = 92.53%: 100%|██████████| 13/13 [00:00<00:00, 32.89it/s]\n",
            "[17 / 50] Train: Loss = 0.19789, Accuracy = 93.72%: 100%|██████████| 572/572 [00:10<00:00, 55.96it/s]\n",
            "[17 / 50]   Val: Loss = 0.22180, Accuracy = 92.76%: 100%|██████████| 13/13 [00:00<00:00, 33.50it/s]\n",
            "[18 / 50] Train: Loss = 0.19518, Accuracy = 93.85%: 100%|██████████| 572/572 [00:10<00:00, 56.04it/s]\n",
            "[18 / 50]   Val: Loss = 0.23338, Accuracy = 92.37%: 100%|██████████| 13/13 [00:00<00:00, 34.09it/s]\n",
            "[19 / 50] Train: Loss = 0.19109, Accuracy = 93.98%: 100%|██████████| 572/572 [00:10<00:00, 55.84it/s]\n",
            "[19 / 50]   Val: Loss = 0.23615, Accuracy = 92.22%: 100%|██████████| 13/13 [00:00<00:00, 33.36it/s]\n",
            "[20 / 50] Train: Loss = 0.18974, Accuracy = 94.01%: 100%|██████████| 572/572 [00:10<00:00, 56.15it/s]\n",
            "[20 / 50]   Val: Loss = 0.21881, Accuracy = 92.90%: 100%|██████████| 13/13 [00:00<00:00, 32.46it/s]\n",
            "[21 / 50] Train: Loss = 0.18658, Accuracy = 94.13%: 100%|██████████| 572/572 [00:10<00:00, 56.15it/s]\n",
            "[21 / 50]   Val: Loss = 0.22617, Accuracy = 92.66%: 100%|██████████| 13/13 [00:00<00:00, 32.42it/s]\n",
            "[22 / 50] Train: Loss = 0.18502, Accuracy = 94.22%: 100%|██████████| 572/572 [00:10<00:00, 56.32it/s]\n",
            "[22 / 50]   Val: Loss = 0.21951, Accuracy = 92.94%: 100%|██████████| 13/13 [00:00<00:00, 33.24it/s]\n",
            "[23 / 50] Train: Loss = 0.18263, Accuracy = 94.25%: 100%|██████████| 572/572 [00:10<00:00, 56.56it/s]\n",
            "[23 / 50]   Val: Loss = 0.22810, Accuracy = 92.51%: 100%|██████████| 13/13 [00:00<00:00, 33.74it/s]\n",
            "[24 / 50] Train: Loss = 0.18081, Accuracy = 94.31%: 100%|██████████| 572/572 [00:10<00:00, 56.05it/s]\n",
            "[24 / 50]   Val: Loss = 0.22537, Accuracy = 92.57%: 100%|██████████| 13/13 [00:00<00:00, 32.92it/s]\n",
            "[25 / 50] Train: Loss = 0.17893, Accuracy = 94.39%: 100%|██████████| 572/572 [00:10<00:00, 56.37it/s]\n",
            "[25 / 50]   Val: Loss = 0.21510, Accuracy = 92.96%: 100%|██████████| 13/13 [00:00<00:00, 34.00it/s]\n",
            "[26 / 50] Train: Loss = 0.17818, Accuracy = 94.42%: 100%|██████████| 572/572 [00:10<00:00, 55.98it/s]\n",
            "[26 / 50]   Val: Loss = 0.20889, Accuracy = 93.19%: 100%|██████████| 13/13 [00:00<00:00, 34.08it/s]\n",
            "[27 / 50] Train: Loss = 0.17617, Accuracy = 94.45%: 100%|██████████| 572/572 [00:10<00:00, 56.73it/s]\n",
            "[27 / 50]   Val: Loss = 0.21589, Accuracy = 92.96%: 100%|██████████| 13/13 [00:00<00:00, 32.41it/s]\n",
            "[28 / 50] Train: Loss = 0.17590, Accuracy = 94.51%: 100%|██████████| 572/572 [00:10<00:00, 57.29it/s]\n",
            "[28 / 50]   Val: Loss = 0.20589, Accuracy = 93.33%: 100%|██████████| 13/13 [00:00<00:00, 32.46it/s]\n",
            "[29 / 50] Train: Loss = 0.17399, Accuracy = 94.57%: 100%|██████████| 572/572 [00:10<00:00, 56.65it/s]\n",
            "[29 / 50]   Val: Loss = 0.21444, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 32.51it/s]\n",
            "[30 / 50] Train: Loss = 0.17227, Accuracy = 94.61%: 100%|██████████| 572/572 [00:10<00:00, 56.27it/s]\n",
            "[30 / 50]   Val: Loss = 0.20818, Accuracy = 93.30%: 100%|██████████| 13/13 [00:00<00:00, 33.95it/s]\n",
            "[31 / 50] Train: Loss = 0.17082, Accuracy = 94.66%: 100%|██████████| 572/572 [00:10<00:00, 56.05it/s]\n",
            "[31 / 50]   Val: Loss = 0.21215, Accuracy = 93.16%: 100%|██████████| 13/13 [00:00<00:00, 34.72it/s]\n",
            "[32 / 50] Train: Loss = 0.17003, Accuracy = 94.69%: 100%|██████████| 572/572 [00:10<00:00, 56.23it/s]\n",
            "[32 / 50]   Val: Loss = 0.21374, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 33.45it/s]\n",
            "[33 / 50] Train: Loss = 0.16999, Accuracy = 94.71%: 100%|██████████| 572/572 [00:10<00:00, 56.31it/s]\n",
            "[33 / 50]   Val: Loss = 0.21195, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 33.34it/s]\n",
            "[34 / 50] Train: Loss = 0.16837, Accuracy = 94.76%: 100%|██████████| 572/572 [00:10<00:00, 56.10it/s]\n",
            "[34 / 50]   Val: Loss = 0.21555, Accuracy = 92.95%: 100%|██████████| 13/13 [00:00<00:00, 31.71it/s]\n",
            "[35 / 50] Train: Loss = 0.16716, Accuracy = 94.79%: 100%|██████████| 572/572 [00:10<00:00, 56.00it/s]\n",
            "[35 / 50]   Val: Loss = 0.20841, Accuracy = 93.10%: 100%|██████████| 13/13 [00:00<00:00, 32.66it/s]\n",
            "[36 / 50] Train: Loss = 0.16509, Accuracy = 94.86%: 100%|██████████| 572/572 [00:10<00:00, 56.38it/s]\n",
            "[36 / 50]   Val: Loss = 0.20726, Accuracy = 93.30%: 100%|██████████| 13/13 [00:00<00:00, 32.27it/s]\n",
            "[37 / 50] Train: Loss = 0.16493, Accuracy = 94.86%: 100%|██████████| 572/572 [00:10<00:00, 56.36it/s]\n",
            "[37 / 50]   Val: Loss = 0.20788, Accuracy = 93.13%: 100%|██████████| 13/13 [00:00<00:00, 33.69it/s]\n",
            "[38 / 50] Train: Loss = 0.16467, Accuracy = 94.89%: 100%|██████████| 572/572 [00:10<00:00, 55.78it/s]\n",
            "[38 / 50]   Val: Loss = 0.21665, Accuracy = 92.81%: 100%|██████████| 13/13 [00:00<00:00, 33.99it/s]\n",
            "[39 / 50] Train: Loss = 0.16391, Accuracy = 94.91%: 100%|██████████| 572/572 [00:10<00:00, 56.32it/s]\n",
            "[39 / 50]   Val: Loss = 0.20126, Accuracy = 93.50%: 100%|██████████| 13/13 [00:00<00:00, 32.00it/s]\n",
            "[40 / 50] Train: Loss = 0.16331, Accuracy = 94.92%: 100%|██████████| 572/572 [00:10<00:00, 55.93it/s]\n",
            "[40 / 50]   Val: Loss = 0.20139, Accuracy = 93.47%: 100%|██████████| 13/13 [00:00<00:00, 32.91it/s]\n",
            "[41 / 50] Train: Loss = 0.16186, Accuracy = 94.97%: 100%|██████████| 572/572 [00:10<00:00, 56.36it/s]\n",
            "[41 / 50]   Val: Loss = 0.19950, Accuracy = 93.50%: 100%|██████████| 13/13 [00:00<00:00, 33.27it/s]\n",
            "[42 / 50] Train: Loss = 0.16088, Accuracy = 95.02%: 100%|██████████| 572/572 [00:10<00:00, 54.52it/s]\n",
            "[42 / 50]   Val: Loss = 0.20609, Accuracy = 93.35%: 100%|██████████| 13/13 [00:00<00:00, 33.88it/s]\n",
            "[43 / 50] Train: Loss = 0.16078, Accuracy = 95.02%: 100%|██████████| 572/572 [00:10<00:00, 55.98it/s]\n",
            "[43 / 50]   Val: Loss = 0.20812, Accuracy = 93.23%: 100%|██████████| 13/13 [00:00<00:00, 32.33it/s]\n",
            "[44 / 50] Train: Loss = 0.16066, Accuracy = 95.01%: 100%|██████████| 572/572 [00:10<00:00, 55.91it/s]\n",
            "[44 / 50]   Val: Loss = 0.20522, Accuracy = 93.32%: 100%|██████████| 13/13 [00:00<00:00, 33.50it/s]\n",
            "[45 / 50] Train: Loss = 0.16000, Accuracy = 95.03%: 100%|██████████| 572/572 [00:10<00:00, 56.31it/s]\n",
            "[45 / 50]   Val: Loss = 0.20559, Accuracy = 93.17%: 100%|██████████| 13/13 [00:00<00:00, 32.91it/s]\n",
            "[46 / 50] Train: Loss = 0.15937, Accuracy = 95.08%: 100%|██████████| 572/572 [00:10<00:00, 56.19it/s]\n",
            "[46 / 50]   Val: Loss = 0.20736, Accuracy = 93.14%: 100%|██████████| 13/13 [00:00<00:00, 33.29it/s]\n",
            "[47 / 50] Train: Loss = 0.15778, Accuracy = 95.14%: 100%|██████████| 572/572 [00:10<00:00, 56.16it/s]\n",
            "[47 / 50]   Val: Loss = 0.20143, Accuracy = 93.44%: 100%|██████████| 13/13 [00:00<00:00, 34.64it/s]\n",
            "[48 / 50] Train: Loss = 0.15798, Accuracy = 95.13%: 100%|██████████| 572/572 [00:10<00:00, 56.10it/s]\n",
            "[48 / 50]   Val: Loss = 0.20468, Accuracy = 93.34%: 100%|██████████| 13/13 [00:00<00:00, 32.66it/s]\n",
            "[49 / 50] Train: Loss = 0.15761, Accuracy = 95.13%: 100%|██████████| 572/572 [00:10<00:00, 56.29it/s]\n",
            "[49 / 50]   Val: Loss = 0.19695, Accuracy = 93.65%: 100%|██████████| 13/13 [00:00<00:00, 32.16it/s]\n",
            "[50 / 50] Train: Loss = 0.15757, Accuracy = 95.13%: 100%|██████████| 572/572 [00:10<00:00, 59.14it/s]\n",
            "[50 / 50]   Val: Loss = 0.20394, Accuracy = 93.41%: 100%|██████████| 13/13 [00:00<00:00, 32.61it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn5-uOWpH7eS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cc17bca-c5fe-4d26-b18c-3bb61fe69c9d"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay=1e-4)\n",
        "fit(model, criterion, optimizer, scheduler, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.12709, Accuracy = 96.35%: 100%|██████████| 572/572 [00:10<00:00, 55.55it/s]\n",
            "[1 / 50]   Val: Loss = 0.17997, Accuracy = 94.24%: 100%|██████████| 13/13 [00:00<00:00, 34.13it/s]\n",
            "[2 / 50] Train: Loss = 0.12460, Accuracy = 96.43%: 100%|██████████| 572/572 [00:10<00:00, 56.75it/s]\n",
            "[2 / 50]   Val: Loss = 0.18151, Accuracy = 94.18%: 100%|██████████| 13/13 [00:00<00:00, 33.07it/s]\n",
            "[3 / 50] Train: Loss = 0.12343, Accuracy = 96.45%: 100%|██████████| 572/572 [00:09<00:00, 57.20it/s]\n",
            "[3 / 50]   Val: Loss = 0.18108, Accuracy = 94.17%: 100%|██████████| 13/13 [00:00<00:00, 33.98it/s]\n",
            "[4 / 50] Train: Loss = 0.12417, Accuracy = 96.46%: 100%|██████████| 572/572 [00:10<00:00, 56.33it/s]\n",
            "[4 / 50]   Val: Loss = 0.18224, Accuracy = 94.09%: 100%|██████████| 13/13 [00:00<00:00, 33.32it/s]\n",
            "[5 / 50] Train: Loss = 0.12352, Accuracy = 96.46%: 100%|██████████| 572/572 [00:10<00:00, 56.97it/s]\n",
            "[5 / 50]   Val: Loss = 0.18294, Accuracy = 94.07%: 100%|██████████| 13/13 [00:00<00:00, 32.74it/s]\n",
            "[6 / 50] Train: Loss = 0.12419, Accuracy = 96.45%: 100%|██████████| 572/572 [00:10<00:00, 56.41it/s]\n",
            "[6 / 50]   Val: Loss = 0.18177, Accuracy = 94.18%: 100%|██████████| 13/13 [00:00<00:00, 33.79it/s]\n",
            "[7 / 50] Train: Loss = 0.12285, Accuracy = 96.49%: 100%|██████████| 572/572 [00:10<00:00, 57.14it/s]\n",
            "[7 / 50]   Val: Loss = 0.18211, Accuracy = 94.14%: 100%|██████████| 13/13 [00:00<00:00, 33.47it/s]\n",
            "[8 / 50] Train: Loss = 0.12373, Accuracy = 96.47%: 100%|██████████| 572/572 [00:09<00:00, 57.43it/s]\n",
            "[8 / 50]   Val: Loss = 0.18203, Accuracy = 94.16%: 100%|██████████| 13/13 [00:00<00:00, 32.71it/s]\n",
            "[9 / 50] Train: Loss = 0.12262, Accuracy = 96.52%: 100%|██████████| 572/572 [00:10<00:00, 57.02it/s]\n",
            "[9 / 50]   Val: Loss = 0.18305, Accuracy = 94.11%: 100%|██████████| 13/13 [00:00<00:00, 33.01it/s]\n",
            "[10 / 50] Train: Loss = 0.12208, Accuracy = 96.51%: 100%|██████████| 572/572 [00:10<00:00, 56.40it/s]\n",
            "[10 / 50]   Val: Loss = 0.18529, Accuracy = 94.04%: 100%|██████████| 13/13 [00:00<00:00, 33.93it/s]\n",
            "[11 / 50] Train: Loss = 0.12234, Accuracy = 96.52%: 100%|██████████| 572/572 [00:10<00:00, 56.61it/s]\n",
            "[11 / 50]   Val: Loss = 0.18173, Accuracy = 94.16%: 100%|██████████| 13/13 [00:00<00:00, 33.81it/s]\n",
            "[12 / 50] Train: Loss = 0.12219, Accuracy = 96.51%: 100%|██████████| 572/572 [00:10<00:00, 56.32it/s]\n",
            "[12 / 50]   Val: Loss = 0.18429, Accuracy = 94.05%: 100%|██████████| 13/13 [00:00<00:00, 35.47it/s]\n",
            "[13 / 50] Train: Loss = 0.12277, Accuracy = 96.52%: 100%|██████████| 572/572 [00:10<00:00, 56.03it/s]\n",
            "[13 / 50]   Val: Loss = 0.18357, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 33.80it/s]\n",
            "[14 / 50] Train: Loss = 0.12292, Accuracy = 96.50%: 100%|██████████| 572/572 [00:10<00:00, 55.97it/s]\n",
            "[14 / 50]   Val: Loss = 0.18206, Accuracy = 94.19%: 100%|██████████| 13/13 [00:00<00:00, 33.49it/s]\n",
            "[15 / 50] Train: Loss = 0.12248, Accuracy = 96.51%: 100%|██████████| 572/572 [00:10<00:00, 56.49it/s]\n",
            "[15 / 50]   Val: Loss = 0.18212, Accuracy = 94.15%: 100%|██████████| 13/13 [00:00<00:00, 32.78it/s]\n",
            "[16 / 50] Train: Loss = 0.12291, Accuracy = 96.53%: 100%|██████████| 572/572 [00:10<00:00, 54.34it/s]\n",
            "[16 / 50]   Val: Loss = 0.18084, Accuracy = 94.22%: 100%|██████████| 13/13 [00:00<00:00, 33.79it/s]\n",
            "[17 / 50] Train: Loss = 0.12258, Accuracy = 96.50%: 100%|██████████| 572/572 [00:10<00:00, 56.03it/s]\n",
            "[17 / 50]   Val: Loss = 0.18412, Accuracy = 94.09%: 100%|██████████| 13/13 [00:00<00:00, 32.71it/s]\n",
            "[18 / 50] Train: Loss = 0.12202, Accuracy = 96.53%: 100%|██████████| 572/572 [00:10<00:00, 56.01it/s]\n",
            "[18 / 50]   Val: Loss = 0.18058, Accuracy = 94.19%: 100%|██████████| 13/13 [00:00<00:00, 33.70it/s]\n",
            "[19 / 50] Train: Loss = 0.12193, Accuracy = 96.54%: 100%|██████████| 572/572 [00:10<00:00, 56.57it/s]\n",
            "[19 / 50]   Val: Loss = 0.18371, Accuracy = 94.07%: 100%|██████████| 13/13 [00:00<00:00, 32.69it/s]\n",
            "[20 / 50] Train: Loss = 0.12240, Accuracy = 96.53%: 100%|██████████| 572/572 [00:10<00:00, 56.97it/s]\n",
            "[20 / 50]   Val: Loss = 0.18308, Accuracy = 94.09%: 100%|██████████| 13/13 [00:00<00:00, 33.57it/s]\n",
            "[21 / 50] Train: Loss = 0.12299, Accuracy = 96.53%: 100%|██████████| 572/572 [00:10<00:00, 55.88it/s]\n",
            "[21 / 50]   Val: Loss = 0.18456, Accuracy = 94.08%: 100%|██████████| 13/13 [00:00<00:00, 33.67it/s]\n",
            "[22 / 50] Train: Loss = 0.12155, Accuracy = 96.54%: 100%|██████████| 572/572 [00:10<00:00, 56.08it/s]\n",
            "[22 / 50]   Val: Loss = 0.18383, Accuracy = 94.14%: 100%|██████████| 13/13 [00:00<00:00, 34.94it/s]\n",
            "[23 / 50] Train: Loss = 0.12184, Accuracy = 96.53%: 100%|██████████| 572/572 [00:10<00:00, 56.33it/s]\n",
            "[23 / 50]   Val: Loss = 0.18243, Accuracy = 94.15%: 100%|██████████| 13/13 [00:00<00:00, 33.20it/s]\n",
            "[24 / 50] Train: Loss = 0.12169, Accuracy = 96.54%: 100%|██████████| 572/572 [00:10<00:00, 56.27it/s]\n",
            "[24 / 50]   Val: Loss = 0.18334, Accuracy = 94.08%: 100%|██████████| 13/13 [00:00<00:00, 33.14it/s]\n",
            "[25 / 50] Train: Loss = 0.12161, Accuracy = 96.54%: 100%|██████████| 572/572 [00:10<00:00, 56.35it/s]\n",
            "[25 / 50]   Val: Loss = 0.18337, Accuracy = 94.10%: 100%|██████████| 13/13 [00:00<00:00, 33.39it/s]\n",
            "[26 / 50] Train: Loss = 0.12191, Accuracy = 96.54%: 100%|██████████| 572/572 [00:10<00:00, 56.11it/s]\n",
            "[26 / 50]   Val: Loss = 0.18449, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 32.80it/s]\n",
            "[27 / 50] Train: Loss = 0.12185, Accuracy = 96.55%: 100%|██████████| 572/572 [00:10<00:00, 56.26it/s]\n",
            "[27 / 50]   Val: Loss = 0.18483, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 32.22it/s]\n",
            "[28 / 50] Train: Loss = 0.12100, Accuracy = 96.56%: 100%|██████████| 572/572 [00:10<00:00, 56.03it/s]\n",
            "[28 / 50]   Val: Loss = 0.18420, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 33.85it/s]\n",
            "[29 / 50] Train: Loss = 0.12135, Accuracy = 96.55%: 100%|██████████| 572/572 [00:10<00:00, 57.22it/s]\n",
            "[29 / 50]   Val: Loss = 0.18533, Accuracy = 94.07%: 100%|██████████| 13/13 [00:00<00:00, 34.11it/s]\n",
            "[30 / 50] Train: Loss = 0.12203, Accuracy = 96.56%: 100%|██████████| 572/572 [00:10<00:00, 56.05it/s]\n",
            "[30 / 50]   Val: Loss = 0.18332, Accuracy = 94.09%: 100%|██████████| 13/13 [00:00<00:00, 32.78it/s]\n",
            "[31 / 50] Train: Loss = 0.12163, Accuracy = 96.54%: 100%|██████████| 572/572 [00:10<00:00, 56.35it/s]\n",
            "[31 / 50]   Val: Loss = 0.18274, Accuracy = 94.13%: 100%|██████████| 13/13 [00:00<00:00, 35.28it/s]\n",
            "[32 / 50] Train: Loss = 0.12170, Accuracy = 96.56%: 100%|██████████| 572/572 [00:10<00:00, 56.10it/s]\n",
            "[32 / 50]   Val: Loss = 0.18634, Accuracy = 94.00%: 100%|██████████| 13/13 [00:00<00:00, 32.16it/s]\n",
            "[33 / 50] Train: Loss = 0.12110, Accuracy = 96.55%: 100%|██████████| 572/572 [00:10<00:00, 55.85it/s]\n",
            "[33 / 50]   Val: Loss = 0.18566, Accuracy = 94.02%: 100%|██████████| 13/13 [00:00<00:00, 33.14it/s]\n",
            "[34 / 50] Train: Loss = 0.12112, Accuracy = 96.58%: 100%|██████████| 572/572 [00:10<00:00, 56.05it/s]\n",
            "[34 / 50]   Val: Loss = 0.17987, Accuracy = 94.19%: 100%|██████████| 13/13 [00:00<00:00, 33.22it/s]\n",
            "[35 / 50] Train: Loss = 0.12081, Accuracy = 96.58%: 100%|██████████| 572/572 [00:10<00:00, 56.02it/s]\n",
            "[35 / 50]   Val: Loss = 0.18459, Accuracy = 94.04%: 100%|██████████| 13/13 [00:00<00:00, 34.19it/s]\n",
            "[36 / 50] Train: Loss = 0.12121, Accuracy = 96.56%: 100%|██████████| 572/572 [00:10<00:00, 56.14it/s]\n",
            "[36 / 50]   Val: Loss = 0.18396, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 33.04it/s]\n",
            "[37 / 50] Train: Loss = 0.12174, Accuracy = 96.56%: 100%|██████████| 572/572 [00:10<00:00, 56.01it/s]\n",
            "[37 / 50]   Val: Loss = 0.18843, Accuracy = 93.91%: 100%|██████████| 13/13 [00:00<00:00, 32.63it/s]\n",
            "[38 / 50] Train: Loss = 0.12111, Accuracy = 96.56%: 100%|██████████| 572/572 [00:10<00:00, 55.92it/s]\n",
            "[38 / 50]   Val: Loss = 0.18332, Accuracy = 94.09%: 100%|██████████| 13/13 [00:00<00:00, 33.03it/s]\n",
            "[39 / 50] Train: Loss = 0.12075, Accuracy = 96.60%: 100%|██████████| 572/572 [00:10<00:00, 56.19it/s]\n",
            "[39 / 50]   Val: Loss = 0.18463, Accuracy = 94.07%: 100%|██████████| 13/13 [00:00<00:00, 33.63it/s]\n",
            "[40 / 50] Train: Loss = 0.12093, Accuracy = 96.57%: 100%|██████████| 572/572 [00:10<00:00, 56.29it/s]\n",
            "[40 / 50]   Val: Loss = 0.18472, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 32.87it/s]\n",
            "[41 / 50] Train: Loss = 0.12049, Accuracy = 96.58%: 100%|██████████| 572/572 [00:10<00:00, 56.19it/s]\n",
            "[41 / 50]   Val: Loss = 0.18394, Accuracy = 94.08%: 100%|██████████| 13/13 [00:00<00:00, 31.93it/s]\n",
            "[42 / 50] Train: Loss = 0.12086, Accuracy = 96.57%: 100%|██████████| 572/572 [00:10<00:00, 56.05it/s]\n",
            "[42 / 50]   Val: Loss = 0.18498, Accuracy = 94.00%: 100%|██████████| 13/13 [00:00<00:00, 35.24it/s]\n",
            "[43 / 50] Train: Loss = 0.12089, Accuracy = 96.57%: 100%|██████████| 572/572 [00:10<00:00, 56.64it/s]\n",
            "[43 / 50]   Val: Loss = 0.18461, Accuracy = 94.04%: 100%|██████████| 13/13 [00:00<00:00, 34.22it/s]\n",
            "[44 / 50] Train: Loss = 0.12037, Accuracy = 96.59%: 100%|██████████| 572/572 [00:10<00:00, 56.77it/s]\n",
            "[44 / 50]   Val: Loss = 0.18555, Accuracy = 94.04%: 100%|██████████| 13/13 [00:00<00:00, 32.02it/s]\n",
            "[45 / 50] Train: Loss = 0.12024, Accuracy = 96.59%: 100%|██████████| 572/572 [00:10<00:00, 56.84it/s]\n",
            "[45 / 50]   Val: Loss = 0.18387, Accuracy = 94.07%: 100%|██████████| 13/13 [00:00<00:00, 33.47it/s]\n",
            "[46 / 50] Train: Loss = 0.11999, Accuracy = 96.61%: 100%|██████████| 572/572 [00:10<00:00, 56.55it/s]\n",
            "[46 / 50]   Val: Loss = 0.18730, Accuracy = 93.95%: 100%|██████████| 13/13 [00:00<00:00, 33.00it/s]\n",
            "[47 / 50] Train: Loss = 0.12042, Accuracy = 96.60%: 100%|██████████| 572/572 [00:09<00:00, 57.29it/s]\n",
            "[47 / 50]   Val: Loss = 0.18455, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 32.73it/s]\n",
            "[48 / 50] Train: Loss = 0.12082, Accuracy = 96.59%: 100%|██████████| 572/572 [00:09<00:00, 57.31it/s]\n",
            "[48 / 50]   Val: Loss = 0.18786, Accuracy = 93.93%: 100%|██████████| 13/13 [00:00<00:00, 35.12it/s]\n",
            "[49 / 50] Train: Loss = 0.11962, Accuracy = 96.62%: 100%|██████████| 572/572 [00:10<00:00, 58.04it/s]\n",
            "[49 / 50]   Val: Loss = 0.18585, Accuracy = 94.00%: 100%|██████████| 13/13 [00:00<00:00, 34.20it/s]\n",
            "[50 / 50] Train: Loss = 0.11939, Accuracy = 96.63%: 100%|██████████| 572/572 [00:10<00:00, 53.58it/s]\n",
            "[50 / 50]   Val: Loss = 0.18464, Accuracy = 94.08%: 100%|██████████| 13/13 [00:00<00:00, 33.26it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98wr38_rw55D",
        "outputId": "a05462ec-7084-42ef-b4bd-a9eaab3f4e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_loss, test_acc = do_epoch(model, criterion, data = (X_test, y_test), batch_size = 64)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Loss = 0.19496, Accuracy = 93.84%: 100%|██████████| 224/224 [00:02<00:00, 98.12it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKNyLW030p6w",
        "colab_type": "code",
        "outputId": "89b7eeac-7975-42ec-af51-d77e174b8b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(test_loss, test_acc)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1949646504861968 0.9383686383056368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywRU6jLXe4jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "outputId": "bc2e7a1f-72d2-416d-e9b5-8f5d8e7210ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0zXVCrnuFJY",
        "colab_type": "code",
        "outputId": "6c8bc85a-a51e-41fb-e120-e0bab54190fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "w2v_model.most_similar(\"cat\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dog', 0.8798074722290039),\n",
              " ('rabbit', 0.7424426674842834),\n",
              " ('cats', 0.7323004007339478),\n",
              " ('monkey', 0.7288709878921509),\n",
              " ('pet', 0.7190139889717102),\n",
              " ('dogs', 0.7163872718811035),\n",
              " ('mouse', 0.6915250420570374),\n",
              " ('puppy', 0.6800068020820618),\n",
              " ('rat', 0.6641027331352234),\n",
              " ('spider', 0.6501135230064392)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfIwer2quW54",
        "colab_type": "code",
        "outputId": "f376f1b2-442c-41d9-c216-9830a2428ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "w2v_model.vectors.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "outputId": "37f85550-27a0-4ed7-d258-0e147a99e434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "known_count = 0\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfa4674d-5784-4874-abbc-0b0f0193c1d4"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind),\n",
        "    embeddings = torch.tensor(embeddings).to(device).type(FloatTensor)\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1)\n",
        "\n",
        "fit(model, criterion, optimizer, scheduler, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.49325, Accuracy = 86.60%: 100%|██████████| 572/572 [00:10<00:00, 55.89it/s]\n",
            "[1 / 50]   Val: Loss = 0.22910, Accuracy = 93.10%: 100%|██████████| 13/13 [00:00<00:00, 33.72it/s]\n",
            "[2 / 50] Train: Loss = 0.17906, Accuracy = 94.60%: 100%|██████████| 572/572 [00:10<00:00, 56.21it/s]\n",
            "[2 / 50]   Val: Loss = 0.16406, Accuracy = 95.06%: 100%|██████████| 13/13 [00:00<00:00, 32.75it/s]\n",
            "[3 / 50] Train: Loss = 0.12902, Accuracy = 96.07%: 100%|██████████| 572/572 [00:10<00:00, 56.11it/s]\n",
            "[3 / 50]   Val: Loss = 0.13553, Accuracy = 95.86%: 100%|██████████| 13/13 [00:00<00:00, 32.53it/s]\n",
            "[4 / 50] Train: Loss = 0.10522, Accuracy = 96.80%: 100%|██████████| 572/572 [00:10<00:00, 56.46it/s]\n",
            "[4 / 50]   Val: Loss = 0.12312, Accuracy = 96.18%: 100%|██████████| 13/13 [00:00<00:00, 34.29it/s]\n",
            "[5 / 50] Train: Loss = 0.09146, Accuracy = 97.18%: 100%|██████████| 572/572 [00:10<00:00, 56.26it/s]\n",
            "[5 / 50]   Val: Loss = 0.11586, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 34.01it/s]\n",
            "[6 / 50] Train: Loss = 0.08048, Accuracy = 97.51%: 100%|██████████| 572/572 [00:10<00:00, 52.33it/s]\n",
            "[6 / 50]   Val: Loss = 0.10579, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 33.63it/s]\n",
            "[7 / 50] Train: Loss = 0.07240, Accuracy = 97.75%: 100%|██████████| 572/572 [00:10<00:00, 56.42it/s]\n",
            "[7 / 50]   Val: Loss = 0.10451, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 32.61it/s]\n",
            "[8 / 50] Train: Loss = 0.06623, Accuracy = 97.92%: 100%|██████████| 572/572 [00:10<00:00, 56.57it/s]\n",
            "[8 / 50]   Val: Loss = 0.10247, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 32.18it/s]\n",
            "[9 / 50] Train: Loss = 0.06123, Accuracy = 98.09%: 100%|██████████| 572/572 [00:10<00:00, 56.57it/s]\n",
            "[9 / 50]   Val: Loss = 0.09803, Accuracy = 97.02%: 100%|██████████| 13/13 [00:00<00:00, 33.91it/s]\n",
            "[10 / 50] Train: Loss = 0.05613, Accuracy = 98.21%: 100%|██████████| 572/572 [00:10<00:00, 56.51it/s]\n",
            "[10 / 50]   Val: Loss = 0.10312, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 33.30it/s]\n",
            "[11 / 50] Train: Loss = 0.05122, Accuracy = 98.38%: 100%|██████████| 572/572 [00:10<00:00, 56.83it/s]\n",
            "[11 / 50]   Val: Loss = 0.10177, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 31.84it/s]\n",
            "[12 / 50] Train: Loss = 0.04769, Accuracy = 98.50%: 100%|██████████| 572/572 [00:10<00:00, 56.72it/s]\n",
            "[12 / 50]   Val: Loss = 0.10348, Accuracy = 97.06%: 100%|██████████| 13/13 [00:00<00:00, 34.70it/s]\n",
            "[13 / 50] Train: Loss = 0.04458, Accuracy = 98.61%: 100%|██████████| 572/572 [00:10<00:00, 56.45it/s]\n",
            "[13 / 50]   Val: Loss = 0.09865, Accuracy = 97.11%: 100%|██████████| 13/13 [00:00<00:00, 34.42it/s]\n",
            "[14 / 50] Train: Loss = 0.04068, Accuracy = 98.72%: 100%|██████████| 572/572 [00:10<00:00, 56.48it/s]\n",
            "[14 / 50]   Val: Loss = 0.11031, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 32.47it/s]\n",
            "[15 / 50] Train: Loss = 0.03791, Accuracy = 98.81%: 100%|██████████| 572/572 [00:10<00:00, 56.01it/s]\n",
            "[15 / 50]   Val: Loss = 0.10402, Accuracy = 97.09%: 100%|██████████| 13/13 [00:00<00:00, 33.25it/s]\n",
            "[16 / 50] Train: Loss = 0.03459, Accuracy = 98.94%: 100%|██████████| 572/572 [00:10<00:00, 56.19it/s]\n",
            "[16 / 50]   Val: Loss = 0.11060, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 32.57it/s]\n",
            "[17 / 50] Train: Loss = 0.03156, Accuracy = 99.02%: 100%|██████████| 572/572 [00:10<00:00, 56.28it/s]\n",
            "[17 / 50]   Val: Loss = 0.11127, Accuracy = 97.06%: 100%|██████████| 13/13 [00:00<00:00, 32.86it/s]\n",
            "[18 / 50] Train: Loss = 0.02864, Accuracy = 99.11%: 100%|██████████| 572/572 [00:10<00:00, 56.42it/s]\n",
            "[18 / 50]   Val: Loss = 0.11188, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 32.78it/s]\n",
            "[19 / 50] Train: Loss = 0.02634, Accuracy = 99.19%: 100%|██████████| 572/572 [00:10<00:00, 56.21it/s]\n",
            "[19 / 50]   Val: Loss = 0.11409, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 33.68it/s]\n",
            "[20 / 50] Train: Loss = 0.02467, Accuracy = 99.25%: 100%|██████████| 572/572 [00:10<00:00, 56.30it/s]\n",
            "[20 / 50]   Val: Loss = 0.11907, Accuracy = 97.02%: 100%|██████████| 13/13 [00:00<00:00, 32.64it/s]\n",
            "[21 / 50] Train: Loss = 0.02299, Accuracy = 99.32%: 100%|██████████| 572/572 [00:10<00:00, 56.33it/s]\n",
            "[21 / 50]   Val: Loss = 0.11796, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 34.08it/s]\n",
            "[22 / 50] Train: Loss = 0.02120, Accuracy = 99.36%: 100%|██████████| 572/572 [00:10<00:00, 56.47it/s]\n",
            "[22 / 50]   Val: Loss = 0.12533, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 33.49it/s]\n",
            "[23 / 50] Train: Loss = 0.01947, Accuracy = 99.44%: 100%|██████████| 572/572 [00:10<00:00, 56.53it/s]\n",
            "[23 / 50]   Val: Loss = 0.12653, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 31.64it/s]\n",
            "[24 / 50] Train: Loss = 0.01963, Accuracy = 99.41%: 100%|██████████| 572/572 [00:10<00:00, 56.39it/s]\n",
            "[24 / 50]   Val: Loss = 0.13137, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 32.73it/s]\n",
            "[25 / 50] Train: Loss = 0.01655, Accuracy = 99.52%: 100%|██████████| 572/572 [00:10<00:00, 56.46it/s]\n",
            "[25 / 50]   Val: Loss = 0.13864, Accuracy = 96.95%: 100%|██████████| 13/13 [00:00<00:00, 33.56it/s]\n",
            "[26 / 50] Train: Loss = 0.01534, Accuracy = 99.55%: 100%|██████████| 572/572 [00:10<00:00, 56.58it/s]\n",
            "[26 / 50]   Val: Loss = 0.14395, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 34.41it/s]\n",
            "[27 / 50] Train: Loss = 0.01424, Accuracy = 99.59%: 100%|██████████| 572/572 [00:10<00:00, 56.07it/s]\n",
            "[27 / 50]   Val: Loss = 0.13605, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 31.59it/s]\n",
            "[28 / 50] Train: Loss = 0.01426, Accuracy = 99.58%: 100%|██████████| 572/572 [00:10<00:00, 55.97it/s]\n",
            "[28 / 50]   Val: Loss = 0.14849, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 31.90it/s]\n",
            "[29 / 50] Train: Loss = 0.01366, Accuracy = 99.61%: 100%|██████████| 572/572 [00:10<00:00, 56.45it/s]\n",
            "[29 / 50]   Val: Loss = 0.15121, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 32.47it/s]\n",
            "[30 / 50] Train: Loss = 0.01290, Accuracy = 99.60%: 100%|██████████| 572/572 [00:10<00:00, 56.31it/s]\n",
            "[30 / 50]   Val: Loss = 0.15003, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 33.30it/s]\n",
            "[31 / 50] Train: Loss = 0.01146, Accuracy = 99.66%: 100%|██████████| 572/572 [00:10<00:00, 56.26it/s]\n",
            "[31 / 50]   Val: Loss = 0.15677, Accuracy = 96.95%: 100%|██████████| 13/13 [00:00<00:00, 33.26it/s]\n",
            "[32 / 50] Train: Loss = 0.01131, Accuracy = 99.66%: 100%|██████████| 572/572 [00:10<00:00, 56.42it/s]\n",
            "[32 / 50]   Val: Loss = 0.15864, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 32.35it/s]\n",
            "[33 / 50] Train: Loss = 0.01125, Accuracy = 99.65%: 100%|██████████| 572/572 [00:10<00:00, 56.37it/s]\n",
            "[33 / 50]   Val: Loss = 0.15760, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 33.40it/s]\n",
            "[34 / 50] Train: Loss = 0.01048, Accuracy = 99.69%: 100%|██████████| 572/572 [00:10<00:00, 56.50it/s]\n",
            "[34 / 50]   Val: Loss = 0.16170, Accuracy = 96.88%: 100%|██████████| 13/13 [00:00<00:00, 33.13it/s]\n",
            "[35 / 50] Train: Loss = 0.01119, Accuracy = 99.65%: 100%|██████████| 572/572 [00:10<00:00, 56.43it/s]\n",
            "[35 / 50]   Val: Loss = 0.16599, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 34.87it/s]\n",
            "[36 / 50] Train: Loss = 0.01018, Accuracy = 99.68%: 100%|██████████| 572/572 [00:10<00:00, 56.47it/s]\n",
            "[36 / 50]   Val: Loss = 0.16103, Accuracy = 96.88%: 100%|██████████| 13/13 [00:00<00:00, 32.33it/s]\n",
            "[37 / 50] Train: Loss = 0.00867, Accuracy = 99.72%: 100%|██████████| 572/572 [00:10<00:00, 56.38it/s]\n",
            "[37 / 50]   Val: Loss = 0.16838, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 33.40it/s]\n",
            "[38 / 50] Train: Loss = 0.00851, Accuracy = 99.73%: 100%|██████████| 572/572 [00:10<00:00, 56.30it/s]\n",
            "[38 / 50]   Val: Loss = 0.17016, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 34.83it/s]\n",
            "[39 / 50] Train: Loss = 0.00964, Accuracy = 99.69%: 100%|██████████| 572/572 [00:10<00:00, 56.54it/s]\n",
            "[39 / 50]   Val: Loss = 0.17053, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 32.65it/s]\n",
            "[40 / 50] Train: Loss = 0.00923, Accuracy = 99.70%: 100%|██████████| 572/572 [00:10<00:00, 56.34it/s]\n",
            "[40 / 50]   Val: Loss = 0.17542, Accuracy = 96.89%: 100%|██████████| 13/13 [00:00<00:00, 32.62it/s]\n",
            "[41 / 50] Train: Loss = 0.00892, Accuracy = 99.71%: 100%|██████████| 572/572 [00:10<00:00, 56.51it/s]\n",
            "[41 / 50]   Val: Loss = 0.18185, Accuracy = 96.89%: 100%|██████████| 13/13 [00:00<00:00, 33.59it/s]\n",
            "[42 / 50] Train: Loss = 0.00901, Accuracy = 99.71%: 100%|██████████| 572/572 [00:10<00:00, 56.88it/s]\n",
            "[42 / 50]   Val: Loss = 0.18964, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 33.10it/s]\n",
            "[43 / 50] Train: Loss = 0.00914, Accuracy = 99.69%: 100%|██████████| 572/572 [00:10<00:00, 56.19it/s]\n",
            "[43 / 50]   Val: Loss = 0.18456, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 34.19it/s]\n",
            "[44 / 50] Train: Loss = 0.00712, Accuracy = 99.76%: 100%|██████████| 572/572 [00:10<00:00, 56.46it/s]\n",
            "[44 / 50]   Val: Loss = 0.18161, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 33.75it/s]\n",
            "[45 / 50] Train: Loss = 0.00792, Accuracy = 99.74%: 100%|██████████| 572/572 [00:10<00:00, 56.39it/s]\n",
            "[45 / 50]   Val: Loss = 0.18878, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 34.45it/s]\n",
            "[46 / 50] Train: Loss = 0.00859, Accuracy = 99.71%: 100%|██████████| 572/572 [00:10<00:00, 56.74it/s]\n",
            "[46 / 50]   Val: Loss = 0.18923, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 33.33it/s]\n",
            "[47 / 50] Train: Loss = 0.00808, Accuracy = 99.74%: 100%|██████████| 572/572 [00:10<00:00, 56.59it/s]\n",
            "[47 / 50]   Val: Loss = 0.18689, Accuracy = 96.88%: 100%|██████████| 13/13 [00:00<00:00, 32.09it/s]\n",
            "[48 / 50] Train: Loss = 0.00971, Accuracy = 99.68%: 100%|██████████| 572/572 [00:10<00:00, 55.39it/s]\n",
            "[48 / 50]   Val: Loss = 0.19487, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 35.29it/s]\n",
            "[49 / 50] Train: Loss = 0.00847, Accuracy = 99.71%: 100%|██████████| 572/572 [00:10<00:00, 56.59it/s]\n",
            "[49 / 50]   Val: Loss = 0.19052, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 33.00it/s]\n",
            "[50 / 50] Train: Loss = 0.00779, Accuracy = 99.74%: 100%|██████████| 572/572 [00:10<00:00, 56.21it/s]\n",
            "[50 / 50]   Val: Loss = 0.19466, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 33.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4d789774-1854-41a9-8845-63cad0935fb1"
      },
      "source": [
        "test_loss, test_acc = do_epoch(model, criterion, data = (X_test, y_test), batch_size = 64)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Loss = 0.20280, Accuracy = 96.63%: 100%|██████████| 224/224 [00:02<00:00, 97.81it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 5.2 - Word2Vec with Negative Sampling\n",
    "\n",
    "В этом задании мы натренируем свои версию word vectors с negative sampling на том же небольшом датасете.\n",
    "\n",
    "\n",
    "Несмотря на то, что основная причина использования Negative Sampling - улучшение скорости тренировки word2vec, в нашем игрушечном примере мы **не требуем** улучшения производительности. Мы используем negative sampling просто как дополнительное упражнение для знакомства с PyTorch.\n",
    "\n",
    "Перед запуском нужно запустить скрипт `download_data.sh`, чтобы скачать данные.\n",
    "\n",
    "Датасет и модель очень небольшие, поэтому это задание можно выполнить и без GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll use Principal Component Analysis (PCA) to visualize word vectors,\n",
    "# so make sure you install dependencies from requirements.txt!\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 19538\n",
      "Num sentences: 11855\n",
      "case ['nature', 'suffers', 'arrested']\n",
      "['intentions', 'sad', 'everything', 'courageous', 'childlike']\n",
      "bleed ['completely', 'dry']\n",
      "['crashing', 'satire', 'full-length', 'undisputed', 'nuanced']\n",
      "chãâ¢teau ['been', 'benefited']\n",
      "['work', 'does', 'appear', 'expands', 'shots']\n",
      "imax ['personal', 'manual']\n",
      "['sudsy', 'convince', 'redemption', 'chloroform-soaked', 'proving']\n",
      "rage ['...', 'blasts', 'later']\n",
      "['proven', 'self-destructive', 'laws', 'borstal', 'contemptible']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "def get_OneHot_by_index(index, num_tokens):\n",
    "    vector = [0]*num_tokens\n",
    "    vector[index] = 1\n",
    "    return torch.tensor(vector)\n",
    "\n",
    "class StanfordTreeBank:\n",
    "    '''\n",
    "    Wrapper for accessing Stanford Tree Bank Dataset\n",
    "    https://nlp.stanford.edu/sentiment/treebank.html\n",
    "    \n",
    "    Parses dataset, gives each token and index and provides lookups\n",
    "    from string token to index and back\n",
    "    \n",
    "    Allows to generate random context with sampling strategy described in\n",
    "    word2vec paper:\n",
    "    https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.index_by_token = {} # map of string -> token index\n",
    "        self.token_by_index = []\n",
    "\n",
    "        self.sentences_by_token = {}\n",
    "        self.sentences = []\n",
    "\n",
    "        self.token_freq = {}\n",
    "        \n",
    "        self.token_reject_by_index = None\n",
    "\n",
    "    def load_dataset(self, folder):\n",
    "        filename = os.path.join(folder, \"datasetSentences.txt\")\n",
    "\n",
    "        with open(filename, \"r\", encoding=\"latin1\") as f:\n",
    "            l = f.readline() # skip the first line\n",
    "            \n",
    "            for l in f:\n",
    "                splitted_line = l.strip().split()\n",
    "                words = [w.lower() for w in splitted_line[1:]] # First one is a number\n",
    "                    \n",
    "                self.sentences.append(words)\n",
    "                for i, word in enumerate(words):\n",
    "                    if word in self.token_freq:\n",
    "                        self.token_freq[word] +=1\n",
    "                        self.sentences_by_token[word].append((len(self.sentences) - 1, i))\n",
    "                    else:\n",
    "                        index = len(self.token_by_index)\n",
    "                        self.token_freq[word] = 1\n",
    "                        self.index_by_token[word] = index\n",
    "                        self.token_by_index.append(word)\n",
    "                        self.sentences_by_token[word] = [(len(self.sentences) - 1, i)] \n",
    "        self.compute_token_prob()\n",
    "                        \n",
    "    def compute_token_prob(self):\n",
    "        words_count = np.array([self.token_freq[token] for token in self.token_by_index])\n",
    "        words_freq = words_count / np.sum(words_count)\n",
    "        \n",
    "        # Following sampling strategy from word2vec paper\n",
    "        self.token_reject_by_index = 1- np.sqrt(1e-5/words_freq)\n",
    "    \n",
    "    def check_reject(self, word):\n",
    "        return np.random.rand() > self.token_reject_by_index[self.index_by_token[word]]\n",
    "        \n",
    "    def get_random_context(self, context_length=5):\n",
    "        \"\"\"\n",
    "        Returns tuple of center word and list of context words\n",
    "        \"\"\"\n",
    "        sentence_sampled = []\n",
    "        while len(sentence_sampled) <= 2:\n",
    "            sentence_index = np.random.randint(len(self.sentences)) \n",
    "            sentence = self.sentences[sentence_index]\n",
    "            sentence_sampled = [word for word in sentence if self.check_reject(word)]\n",
    "    \n",
    "        center_word_index = np.random.randint(len(sentence_sampled))\n",
    "        \n",
    "        words_before = sentence_sampled[max(center_word_index - context_length//2,0):center_word_index]\n",
    "        words_after = sentence_sampled[center_word_index+1: center_word_index+1+context_length//2]\n",
    "        \n",
    "        return sentence_sampled[center_word_index], words_before+words_after\n",
    "    \n",
    "    def num_tokens(self):\n",
    "        return len(self.token_by_index)\n",
    "     \n",
    "    def get_OneHot_by_token(self, token):\n",
    "        return get_OneHot_by_index(self.index_by_token[token], self.num_tokens())\n",
    "    \n",
    "    def find_sentence_with_token(self, token, sentence_index, context_length = 5):\n",
    "        for i, x in enumerate(self.sentences_by_token[token]):\n",
    "            if sentence_index == x[0]:\n",
    "                sentence = [word for word in self.sentences[sentence_index] \\\n",
    "                                if self.check_reject(word)]\n",
    "                begin = max(x[1] - context_length//2, 0)\n",
    "                end = min(x[1] + context_length//2, len(sentence))\n",
    "                sequence = sentence[:begin] + sentence[end+1:]\n",
    "                if sequence != []:\n",
    "                    return random.choice(sequence)\n",
    "        return None\n",
    "\n",
    "    def get_random_out_context(self, token, num_negatives_samples, context_length = 5):\n",
    "        result = []\n",
    "        while len(result) < num_negatives_samples:\n",
    "            sentence_index = np.random.randint(len(self.sentences))\n",
    "            chosen_token = self.find_sentence_with_token(token, sentence_index, context_length)\n",
    "            if chosen_token != None:\n",
    "                result.append(chosen_token)\n",
    "            else:\n",
    "                while True:\n",
    "                    chosen_token = random.choice(self.sentences[sentence_index])\n",
    "                    if self.check_reject(chosen_token):\n",
    "                        result.append(chosen_token)\n",
    "                        break\n",
    "        return result\n",
    "    \n",
    "data = StanfordTreeBank()\n",
    "data.load_dataset(\"./stanfordSentimentTreebank/\")\n",
    "\n",
    "print(\"Num tokens:\", data.num_tokens())\n",
    "print(\"Num sentences:\", len(data.sentences))\n",
    "for i in range(5):\n",
    "    center_word, other_words = data.get_random_context(5)\n",
    "    print(center_word, other_words)\n",
    "    out_context = data.get_random_out_context(center_word, 5, 5)\n",
    "    print(out_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset для Negative Sampling должен быть немного другим\n",
    "\n",
    "Как и прежде, Dataset должен сгенерировать много случайных контекстов и превратить их в сэмплы для тренировки.\n",
    "\n",
    "Здесь мы реализуем прямой проход модели сами, поэтому выдавать данные можно в удобном нам виде.\n",
    "Напоминаем, что в случае negative sampling каждым сэмплом является:\n",
    "- вход: слово в one-hot представлении\n",
    "- выход: набор из одного целевого слова и K других случайных слов из словаря.\n",
    "Вместо softmax + cross-entropy loss, сеть обучается через binary cross-entropy loss - то есть, предсказывает набор бинарных переменных, для каждой из которых функция ошибки считается независимо.\n",
    "\n",
    "Для целевого слова бинарное предсказание должно быть позитивным, а для K случайных слов - негативным.\n",
    "\n",
    "Из набора слово-контекст создается N сэмплов (где N - количество слов в контексте), в каждом из них K+1 целевых слов, для только одного из которых предсказание должно быть позитивным.\n",
    "Например, для K=2:\n",
    "\n",
    "Слово: `orders` и контекст: `['love', 'nicest', 'to', '50-year']` создадут 4 сэмпла:\n",
    "- input: `orders`, target: `[love: 1, any: 0, rose: 0]`\n",
    "- input: `orders`, target: `[nicest: 1, fool: 0, grass: 0]`\n",
    "- input: `orders`, target: `[to: 1, -: 0, the: 0]`\n",
    "- input: `orders`, target: `[50-year: 1, ?: 0, door: 0]`\n",
    "\n",
    "Все слова на входе и на выходе закодированы через one-hot encoding, с размером вектора равным количеству токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample - input: 8337, output indices: tensor([ 1716, 12960,  4750,  8273, 19502, 13561, 14933,  2594, 12896,  1194,\n",
      "          413]), output target: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "num_negative_samples = 10\n",
    "\n",
    "class Word2VecNegativeSampling(Dataset):\n",
    "    '''\n",
    "    PyTorch Dataset for Word2Vec with Negative Sampling.\n",
    "    Accepts StanfordTreebank as data and is able to generate dataset based on\n",
    "    a number of random contexts\n",
    "    '''\n",
    "    def __init__(self, data, num_negative_samples, num_contexts=30000):\n",
    "        '''\n",
    "        Initializes Word2VecNegativeSampling, but doesn't generate the samples yet\n",
    "        (for that, use generate_dataset)\n",
    "        Arguments:\n",
    "        data - StanfordTreebank instace\n",
    "        num_negative_samples - number of negative samples to generate in addition to a positive one\n",
    "        num_contexts - number of random contexts to use when generating a dataset\n",
    "        '''\n",
    "        \n",
    "        # TODO: Implement what you need for other methods!\n",
    "        self.num_contexts = num_contexts\n",
    "        self.data = data\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        \n",
    "    def generate_dataset(self):\n",
    "        '''\n",
    "        Generates dataset samples from random contexts\n",
    "        Note: there will be more samples than contexts because every context\n",
    "        can generate more than one sample\n",
    "        '''\n",
    "        # TODO: Implement generating the dataset\n",
    "        # You should sample num_contexts contexts from the data and turn them into samples\n",
    "        # Note you will have several samples from one context\n",
    "        self.dataset = []\n",
    "        for i in range(self.num_contexts):\n",
    "            center, contexts = self.data.get_random_context()\n",
    "            #print(center, contexts)\n",
    "            for context in contexts:\n",
    "                self.dataset.append((center, [context]+self.data.get_random_out_context(center, \\\n",
    "                                    self.num_negative_samples)))\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns total number of samples\n",
    "        '''\n",
    "        # TODO: Return the number of samples\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns i-th sample\n",
    "        \n",
    "        Return values:\n",
    "        input_vector - index of the input word (not torch.Tensor!)\n",
    "        output_indices - torch.Tensor of indices of the target words. Should be 1+num_negative_samples.\n",
    "        output_target - torch.Tensor with float targets for the training. Should be the same size as output_indices\n",
    "                        and have 1 for the context word and 0 everywhere else\n",
    "        '''\n",
    "        # TODO: Generate tuple of 3 return arguments for i-th sample\n",
    "        center, words = self.dataset[index]\n",
    "        return self.data.index_by_token[center], torch.tensor([self.data.index_by_token[x]\\\n",
    "                for x in words]), torch.tensor([1.]+[0.]*self.num_negative_samples)\n",
    "    \n",
    "dataset = Word2VecNegativeSampling(data, num_negative_samples, 10)\n",
    "dataset.generate_dataset()\n",
    "input_vector, output_indices, output_target = dataset[0]\n",
    "\n",
    "print(\"Sample - input: %s, output indices: %s, output target: %s\" % (int(input_vector), output_indices, output_target)) # target should be able to convert to int\n",
    "assert isinstance(output_indices, torch.Tensor)\n",
    "assert output_indices.shape[0] == num_negative_samples+1\n",
    "\n",
    "assert isinstance(output_target, torch.Tensor)\n",
    "assert output_target.shape[0] == num_negative_samples+1\n",
    "assert torch.sum(output_target) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаем модель\n",
    "\n",
    "Для нашей задачи нам придется реализовать свою собственную PyTorch модель.\n",
    "Эта модель реализует свой собственный прямой проход (forward pass), который получает на вход индекс входного слова и набор индексов для выходных слов. \n",
    "\n",
    "Как всегда, на вход приходит не один сэмпл, а целый batch.  \n",
    "Напомним, что цели улучшить скорость тренировки у нас нет, достаточно чтобы она сходилась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  3,  5],\n",
      "        [ 7,  8, 10]])\n",
      "tensor([[ 95, 110],\n",
      "        [220, 260]])\n",
      "torch.Size([2, 5, 2])\n",
      "torch.Size([2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
    "indices = torch.tensor([1, 2, 4])\n",
    "print(a[:, indices])\n",
    "b = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
    "print(torch.mm(a,b.view(5, 2)))\n",
    "print(torch.stack((a, b), dim = 2).shape)\n",
    "a = torch.tensor([[1], [2]])\n",
    "b = torch.tensor([[3], [4]])\n",
    "print(torch.stack((a, b), dim = 2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2VecNegativeSamples(\n",
       "  (input): Linear(in_features=19538, out_features=10, bias=False)\n",
       "  (output): Linear(in_features=10, out_features=19538, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the usual PyTorch structures\n",
    "dataset = Word2VecNegativeSampling(data, num_negative_samples, 30000)\n",
    "dataset.generate_dataset()\n",
    "\n",
    "# As before, we'll be training very small word vectors!\n",
    "wordvec_dim = 10\n",
    "\n",
    "class Word2VecNegativeSamples(nn.Module):\n",
    "    def __init__(self, num_tokens):\n",
    "        super(Word2VecNegativeSamples, self).__init__()\n",
    "        self.input = nn.Linear(num_tokens, 10, bias=False)\n",
    "        self.output = nn.Linear(10, num_tokens, bias=False)\n",
    "        \n",
    "    def forward(self, input_index_batch, output_indices_batch):\n",
    "        '''\n",
    "        Implements forward pass with negative sampling\n",
    "        \n",
    "        Arguments:\n",
    "        input_index_batch - Tensor of ints, shape: (batch_size, ), indices of input words in the batch\n",
    "        output_indices_batch - Tensor if ints, shape: (batch_size, num_negative_samples+1),\n",
    "                                indices of the target words for every sample\n",
    "                                \n",
    "        Returns:\n",
    "        predictions - Tensor of floats, shape: (batch_size, num_negative_samples+1)\n",
    "        '''\n",
    "        # TODO Implement forward pass\n",
    "        # Hint: You can use for loop to go over all samples on the batch,\n",
    "        # run every sample indivisually and then use\n",
    "        # torch.stack or torch.cat to produce the final result\n",
    "        result = torch.cat(tuple(torch.mm(self.output.weight[output_indices_batch[i], :], \\\n",
    "                                self.input.weight[:, input_index_batch[i]]).view(1, -1) \\\n",
    "                                 for i in range(input_index_batch.shape[0])), dim = 0)\n",
    "        return result\n",
    "    \n",
    "nn_model = Word2VecNegativeSamples(data.num_tokens())\n",
    "nn_model.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6024e-03, -1.3995e-03, -3.7281e-03,  1.0033e-03,  2.9042e-03,\n",
       "         -2.4533e-03,  1.9027e-04,  2.6436e-03, -1.3979e-03, -2.3426e-03],\n",
       "        [-6.7115e-04, -5.9494e-05, -3.3590e-03, -1.1397e-03, -3.2093e-03,\n",
       "          1.6895e-03, -4.8644e-03, -3.3093e-03, -3.8492e-03, -2.5637e-03]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model(torch.tensor([[1], [2]]), torch.tensor([[3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \\\n",
    "                                             [13, 14, 15, 16, 17, 18, 19, 20, 21, 22]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_vectors(nn_model):\n",
    "    '''\n",
    "    Extracts word vectors from the model\n",
    "    \n",
    "    Returns:\n",
    "    input_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
    "    output_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
    "    '''\n",
    "    # TODO: Implement extracting word vectors from param weights\n",
    "    # return tuple of input vectors and output vectos \n",
    "    x1, x2 = nn_model.parameters()\n",
    "    num_tokens = x1.shape[1]\n",
    "    num_dimensions = x1.shape[0]\n",
    "    input_vectors = torch.cat(tuple(x1[:, i].expand(1, -1) for i in range(num_tokens)), dim = 0)\n",
    "    output_vectors = torch.cat(tuple(x2[i, :].expand(1, -1) for i in range(num_tokens)), dim = 0)\n",
    "    return input_vectors, output_vectors\n",
    "\n",
    "untrained_input_vectors, untrained_output_vectors = extract_word_vectors(nn_model)\n",
    "assert untrained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "assert untrained_output_vectors.shape == (data.num_tokens(), wordvec_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neg_sample(model, dataset, train_loader, optimizer, scheduler, num_epochs):    \n",
    "    '''\n",
    "    Trains word2vec with negative samples on and regenerating dataset every epoch\n",
    "    \n",
    "    Returns:\n",
    "    loss_history, train_history\n",
    "    '''\n",
    "    loss = nn.BCEWithLogitsLoss().type(torch.FloatTensor)\n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        dataset.generate_dataset()\n",
    "        num_tokens = model.input.weight.shape[0]\n",
    "\n",
    "        # TODO: Implement training using negative samples\n",
    "        # You can estimate accuracy by comparing prediction values with 0\n",
    "        # And don't forget to step the scheduler!\n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (input_indices, output_indices, targets) in enumerate(train_loader):\n",
    "            predictions = model(input_indices.view(-1,1), output_indices)\n",
    "            loss_value = loss(predictions, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += loss_value\n",
    "            indices = torch.argmax(predictions, 1)\n",
    "            correct_samples += torch.sum(indices == output_indices[:, 0])\n",
    "            total_samples += targets.shape[0]\n",
    "        train_accuracy = float(correct_samples)/ total_samples\n",
    "        ave_loss = loss_accum/(i_step + 1)\n",
    "        print(\"Epoch %i, Average loss: %f, Train accuracy: %f\" % (epoch, ave_loss, train_accuracy))\n",
    "        loss_history.append(ave_loss)\n",
    "        train_history.append(train_accuracy)\n",
    "        scheduler.step()\n",
    "        \n",
    "    return loss_history, train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну и наконец тренировка!\n",
    "\n",
    "Добейтесь значения ошибки меньше **0.25**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average loss: 0.636395, Train accuracy: 0.001025\n",
      "Epoch 1, Average loss: 0.410379, Train accuracy: 0.000664\n",
      "Epoch 2, Average loss: 0.342332, Train accuracy: 0.000928\n",
      "Epoch 3, Average loss: 0.322965, Train accuracy: 0.000942\n",
      "Epoch 4, Average loss: 0.312877, Train accuracy: 0.000930\n",
      "Epoch 5, Average loss: 0.307806, Train accuracy: 0.001003\n",
      "Epoch 6, Average loss: 0.303387, Train accuracy: 0.000725\n",
      "Epoch 7, Average loss: 0.299961, Train accuracy: 0.000870\n",
      "Epoch 8, Average loss: 0.298321, Train accuracy: 0.001050\n",
      "Epoch 9, Average loss: 0.297287, Train accuracy: 0.000735\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's train the model!\n",
    "nn_model = Word2VecNegativeSamples(data.num_tokens())\n",
    "nn_model.type(torch.FloatTensor)\n",
    "# TODO: We use placeholder values for hyperparameters - you will need to find better values!\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=25., weight_decay=0)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n",
    "\n",
    "loss_history, train_history = train_neg_sample(nn_model, dataset, train_loader, optimizer, scheduler, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f765c4ed518>]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training graphs\n",
    "plt.subplot(211)\n",
    "plt.plot(train_history)\n",
    "plt.subplot(212)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average loss: 0.296385, Train accuracy: 0.000882\n",
      "Epoch 1, Average loss: 0.295335, Train accuracy: 0.000895\n",
      "Epoch 2, Average loss: 0.295583, Train accuracy: 0.000688\n"
     ]
    }
   ],
   "source": [
    "# TODO: We use placeholder values for hyperparameters - you will need to find better values!\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=1., weight_decay=0)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n",
    "\n",
    "loss_history, train_history = train_neg_sample(nn_model, dataset, train_loader, optimizer, scheduler, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализируем вектора для разного вида слов до и после тренировки\n",
    "\n",
    "Как и ранее, в случае успешной тренировки вы должны увидеть как вектора слов разных типов (например, знаков препинания, предлогов и остальных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_input_vectors, trained_output_vectors = extract_word_vectors(nn_model)\n",
    "assert trained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "assert trained_output_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "\n",
    "def visualize_vectors(input_vectors, output_vectors, title=''):\n",
    "    full_vectors = torch.cat((input_vectors, output_vectors), 0)\n",
    "    wordvec_embedding = PCA(n_components=2).fit_transform(full_vectors)\n",
    "\n",
    "    # Helpful words form CS244D example\n",
    "    # http://cs224d.stanford.edu/assignment1/index.html\n",
    "    visualize_words = {'green': [\"the\", \"a\", \"an\"], \n",
    "                      'blue': [\",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\"], \n",
    "                      'brown': [\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \n",
    "                              \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\"],\n",
    "                      'orange': [\"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\", \"stupid\"],\n",
    "                      'red': ['tell', 'told', 'said', 'say', 'says', 'tells', 'goes', 'go', 'went']\n",
    "                     }\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.suptitle(title)\n",
    "    for color, words in visualize_words.items():\n",
    "        points = np.array([wordvec_embedding[data.index_by_token[w]] for w in words])\n",
    "        for i, word in enumerate(words):\n",
    "            plt.text(points[i, 0], points[i, 1], word, color=color,horizontalalignment='center')\n",
    "        plt.scatter(points[:, 0], points[:, 1], c=color, alpha=0.3, s=0.5)\n",
    "\n",
    "visualize_vectors(untrained_input_vectors, untrained_output_vectors, \"Untrained word vectors\")\n",
    "visualize_vectors(trained_input_vectors, trained_output_vectors, \"Trained word vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
